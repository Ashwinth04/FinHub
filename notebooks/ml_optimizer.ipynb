{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d3c6bd-81ee-48f7-aea9-c204a8a6c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdcab2a0-3ed8-447a-964c-e69e51a584b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_historical_data(ticker: str):\n",
    "\n",
    "        end_date = datetime.today()\n",
    "        start_date = end_date - timedelta(days=1825)\n",
    "        end_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "        start_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        stock = yf.Ticker(ticker)\n",
    "        hist = stock.history(start=start_str, end=end_str)\n",
    "        hist.reset_index(inplace=True)\n",
    "\n",
    "        return hist\n",
    "\n",
    "        # # Convert dataframe to list of dicts for JSON serialization\n",
    "        # data = hist.to_dict(orient=\"records\")\n",
    "\n",
    "        # filename = f\"{ticker}_{period}_historical.json\"\n",
    "        # with open(filename, \"w\") as f:\n",
    "        #     json.dump(data, f, indent=4, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "437b6a47-4f18-483f-8e7e-bb6d1981fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_historical_data(\"AAPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e291867-71f4-487e-a930-02b578e31202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-26 00:00:00-04:00</td>\n",
       "      <td>78.616088</td>\n",
       "      <td>78.795918</td>\n",
       "      <td>76.914967</td>\n",
       "      <td>76.970863</td>\n",
       "      <td>125522000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-27 00:00:00-04:00</td>\n",
       "      <td>76.827469</td>\n",
       "      <td>77.452018</td>\n",
       "      <td>76.086262</td>\n",
       "      <td>77.306206</td>\n",
       "      <td>112945200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-28 00:00:00-04:00</td>\n",
       "      <td>76.980580</td>\n",
       "      <td>78.601508</td>\n",
       "      <td>76.703544</td>\n",
       "      <td>77.340248</td>\n",
       "      <td>133560800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-29 00:00:00-04:00</td>\n",
       "      <td>77.583260</td>\n",
       "      <td>78.044991</td>\n",
       "      <td>76.907672</td>\n",
       "      <td>77.264908</td>\n",
       "      <td>153532400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-06-01 00:00:00-04:00</td>\n",
       "      <td>77.218738</td>\n",
       "      <td>78.336619</td>\n",
       "      <td>77.087507</td>\n",
       "      <td>78.215111</td>\n",
       "      <td>80791200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>2025-05-15 00:00:00-04:00</td>\n",
       "      <td>210.949997</td>\n",
       "      <td>212.960007</td>\n",
       "      <td>209.539993</td>\n",
       "      <td>211.449997</td>\n",
       "      <td>45029500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>2025-05-16 00:00:00-04:00</td>\n",
       "      <td>212.360001</td>\n",
       "      <td>212.570007</td>\n",
       "      <td>209.770004</td>\n",
       "      <td>211.259995</td>\n",
       "      <td>54737900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>2025-05-19 00:00:00-04:00</td>\n",
       "      <td>207.910004</td>\n",
       "      <td>209.479996</td>\n",
       "      <td>204.259995</td>\n",
       "      <td>208.779999</td>\n",
       "      <td>46140500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>2025-05-20 00:00:00-04:00</td>\n",
       "      <td>207.669998</td>\n",
       "      <td>208.470001</td>\n",
       "      <td>205.029999</td>\n",
       "      <td>206.860001</td>\n",
       "      <td>42496600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>2025-05-21 00:00:00-04:00</td>\n",
       "      <td>205.169998</td>\n",
       "      <td>207.039993</td>\n",
       "      <td>200.710007</td>\n",
       "      <td>202.089996</td>\n",
       "      <td>59134800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1255 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Date        Open        High         Low  \\\n",
       "0    2020-05-26 00:00:00-04:00   78.616088   78.795918   76.914967   \n",
       "1    2020-05-27 00:00:00-04:00   76.827469   77.452018   76.086262   \n",
       "2    2020-05-28 00:00:00-04:00   76.980580   78.601508   76.703544   \n",
       "3    2020-05-29 00:00:00-04:00   77.583260   78.044991   76.907672   \n",
       "4    2020-06-01 00:00:00-04:00   77.218738   78.336619   77.087507   \n",
       "...                        ...         ...         ...         ...   \n",
       "1250 2025-05-15 00:00:00-04:00  210.949997  212.960007  209.539993   \n",
       "1251 2025-05-16 00:00:00-04:00  212.360001  212.570007  209.770004   \n",
       "1252 2025-05-19 00:00:00-04:00  207.910004  209.479996  204.259995   \n",
       "1253 2025-05-20 00:00:00-04:00  207.669998  208.470001  205.029999   \n",
       "1254 2025-05-21 00:00:00-04:00  205.169998  207.039993  200.710007   \n",
       "\n",
       "           Close     Volume  Dividends  Stock Splits  \n",
       "0      76.970863  125522000        0.0           0.0  \n",
       "1      77.306206  112945200        0.0           0.0  \n",
       "2      77.340248  133560800        0.0           0.0  \n",
       "3      77.264908  153532400        0.0           0.0  \n",
       "4      78.215111   80791200        0.0           0.0  \n",
       "...          ...        ...        ...           ...  \n",
       "1250  211.449997   45029500        0.0           0.0  \n",
       "1251  211.259995   54737900        0.0           0.0  \n",
       "1252  208.779999   46140500        0.0           0.0  \n",
       "1253  206.860001   42496600        0.0           0.0  \n",
       "1254  202.089996   59134800        0.0           0.0  \n",
       "\n",
       "[1255 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b67c022f-2f67-477a-b652-e32f4bd946fc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Loading and processing asset data...\n",
      "Processing AAPL...\n",
      "Processing GOOGL...\n",
      "Processing MSFT...\n",
      "Processing TSLA...\n",
      "Processing SPY...\n",
      "Training model...\n",
      "Epoch 0: Train Loss: 0.3769, Val Loss: 0.3755\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline completed successfully!\n",
      "Sample prediction - Asset weights: {'AAPL': 0.204351, 'GOOGL': 0.27352506, 'MSFT': 0.17104423, 'TSLA': 0.22286853, 'SPY': 0.12821111}\n",
      "\n",
      "Example Predictions:\n",
      "Max Sharpe (Risk=3): {'AAPL': 0.205, 'GOOGL': 0.246, 'MSFT': 0.183, 'TSLA': 0.216, 'SPY': 0.151}\n",
      "Max Sharpe (Risk=5): {'AAPL': 0.204, 'GOOGL': 0.274, 'MSFT': 0.171, 'TSLA': 0.223, 'SPY': 0.128}\n",
      "Max Sharpe (Risk=8): {'AAPL': 0.2, 'GOOGL': 0.331, 'MSFT': 0.148, 'TSLA': 0.23, 'SPY': 0.091}\n",
      "Min Volatility (Risk=3): {'AAPL': 0.198, 'GOOGL': 0.201, 'MSFT': 0.201, 'TSLA': 0.198, 'SPY': 0.201}\n",
      "Min Volatility (Risk=5): {'AAPL': 0.199, 'GOOGL': 0.201, 'MSFT': 0.201, 'TSLA': 0.199, 'SPY': 0.201}\n",
      "Min Volatility (Risk=8): {'AAPL': 0.199, 'GOOGL': 0.201, 'MSFT': 0.201, 'TSLA': 0.199, 'SPY': 0.201}\n",
      "Equal Risk Contribution (Risk=3): {'AAPL': 0.199, 'GOOGL': 0.201, 'MSFT': 0.201, 'TSLA': 0.199, 'SPY': 0.201}\n",
      "Equal Risk Contribution (Risk=5): {'AAPL': 0.199, 'GOOGL': 0.201, 'MSFT': 0.2, 'TSLA': 0.199, 'SPY': 0.201}\n",
      "Equal Risk Contribution (Risk=8): {'AAPL': 0.199, 'GOOGL': 0.201, 'MSFT': 0.2, 'TSLA': 0.199, 'SPY': 0.201}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================== DATA PREPROCESSING MODULE ==================\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"Extract features from price data for portfolio optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, lookback_periods: List[int] = [20, 60, 252]):\n",
    "        self.lookback_periods = lookback_periods\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def calculate_returns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate various return metrics\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Simple returns\n",
    "        features['returns_1d'] = df['Close'].pct_change()\n",
    "        features['returns_5d'] = df['Close'].pct_change(5)\n",
    "        features['returns_20d'] = df['Close'].pct_change(20)\n",
    "        \n",
    "        # Log returns\n",
    "        features['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def calculate_volatility(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate volatility measures\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        returns = df['Close'].pct_change()\n",
    "        \n",
    "        for period in self.lookback_periods:\n",
    "            features[f'vol_{period}d'] = returns.rolling(period).std() * np.sqrt(252)\n",
    "            features[f'realized_vol_{period}d'] = returns.rolling(period).std()\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def calculate_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate technical indicators\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Moving averages\n",
    "        for period in [20, 50, 200]:\n",
    "            features[f'sma_{period}'] = df['Close'].rolling(period).mean()\n",
    "            features[f'price_sma_ratio_{period}'] = df['Close'] / features[f'sma_{period}']\n",
    "            \n",
    "        # RSI\n",
    "        delta = df['Close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "        rs = gain / loss\n",
    "        features['rsi'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        sma_20 = df['Close'].rolling(20).mean()\n",
    "        std_20 = df['Close'].rolling(20).std()\n",
    "        features['bb_upper'] = sma_20 + (2 * std_20)\n",
    "        features['bb_lower'] = sma_20 - (2 * std_20)\n",
    "        features['bb_position'] = (df['Close'] - features['bb_lower']) / (features['bb_upper'] - features['bb_lower'])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def calculate_risk_metrics(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate risk-related metrics\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        returns = df['Close'].pct_change()\n",
    "        \n",
    "        # Maximum Drawdown\n",
    "        peak = df['Close'].cummax()\n",
    "        drawdown = (df['Close'] - peak) / peak\n",
    "        features['current_drawdown'] = drawdown\n",
    "        features['max_drawdown_20d'] = drawdown.rolling(20).min()\n",
    "        features['max_drawdown_60d'] = drawdown.rolling(60).min()\n",
    "        \n",
    "        # VaR approximation (assuming normal distribution)\n",
    "        for period in [20, 60]:\n",
    "            vol = returns.rolling(period).std()\n",
    "            mean_ret = returns.rolling(period).mean()\n",
    "            features[f'var_95_{period}d'] = mean_ret - 1.645 * vol\n",
    "            features[f'var_99_{period}d'] = mean_ret - 2.33 * vol\n",
    "            \n",
    "        # Upside/Downside volatility\n",
    "        upside_returns = returns.where(returns > 0, 0)\n",
    "        downside_returns = returns.where(returns < 0, 0)\n",
    "        \n",
    "        features['upside_vol_20d'] = upside_returns.rolling(20).std() * np.sqrt(252)\n",
    "        features['downside_vol_20d'] = downside_returns.rolling(20).std() * np.sqrt(252)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def calculate_momentum_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate momentum indicators\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Price momentum\n",
    "        for period in [20, 60, 252]:\n",
    "            features[f'momentum_{period}d'] = df['Close'] / df['Close'].shift(period) - 1\n",
    "            \n",
    "        # Volume-weighted indicators\n",
    "        if 'Volume' in df.columns:\n",
    "            # Volume-weighted average price\n",
    "            features['vwap_20d'] = (df['Close'] * df['Volume']).rolling(20).sum() / df['Volume'].rolling(20).sum()\n",
    "            features['price_vwap_ratio'] = df['Close'] / features['vwap_20d']\n",
    "            \n",
    "            # Volume momentum\n",
    "            features['volume_sma_20'] = df['Volume'].rolling(20).mean()\n",
    "            features['volume_ratio'] = df['Volume'] / features['volume_sma_20']\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_all_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract all features from a single asset's data\"\"\"\n",
    "        feature_dfs = []\n",
    "        \n",
    "        # Calculate all feature types\n",
    "        feature_dfs.append(self.calculate_returns(df))\n",
    "        feature_dfs.append(self.calculate_volatility(df))\n",
    "        feature_dfs.append(self.calculate_technical_indicators(df))\n",
    "        feature_dfs.append(self.calculate_risk_metrics(df))\n",
    "        feature_dfs.append(self.calculate_momentum_indicators(df))\n",
    "        \n",
    "        # Combine all features\n",
    "        features = pd.concat(feature_dfs, axis=1)\n",
    "        \n",
    "        # Forward fill and backward fill to handle NaN values\n",
    "        features = features.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        # Drop any remaining NaN rows\n",
    "        features = features.dropna()\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "class PortfolioDataProcessor:\n",
    "    \"\"\"Process portfolio data for ML model training\"\"\"\n",
    "    \n",
    "    def __init__(self, assets: List[str], fetch_function, sequence_length: int = 60):\n",
    "        self.assets = assets\n",
    "        self.fetch_function = fetch_function\n",
    "        self.sequence_length = sequence_length\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.scalers = {}\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def load_and_process_data(self) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"Load and process data for all assets\"\"\"\n",
    "        asset_features = {}\n",
    "        \n",
    "        print(\"Loading and processing asset data...\")\n",
    "        for asset in self.assets:\n",
    "            print(f\"Processing {asset}...\")\n",
    "            \n",
    "            # Fetch raw data\n",
    "            df = self.fetch_function(asset)\n",
    "            \n",
    "            # Extract features\n",
    "            features = self.feature_extractor.extract_all_features(df)\n",
    "            \n",
    "            # Store processed features\n",
    "            asset_features[asset] = features\n",
    "            \n",
    "        # Align all assets to the same date range\n",
    "        common_dates = None\n",
    "        for asset, features in asset_features.items():\n",
    "            if common_dates is None:\n",
    "                common_dates = features.index\n",
    "            else:\n",
    "                common_dates = common_dates.intersection(features.index)\n",
    "        \n",
    "        # Create aligned feature matrix\n",
    "        aligned_features = {}\n",
    "        for asset in self.assets:\n",
    "            aligned_features[asset] = asset_features[asset].loc[common_dates]\n",
    "            \n",
    "        # Stack features for all assets\n",
    "        feature_matrix = []\n",
    "        self.feature_names = list(aligned_features[self.assets[0]].columns)\n",
    "        \n",
    "        for i, date in enumerate(common_dates):\n",
    "            if i < self.sequence_length:\n",
    "                continue\n",
    "                \n",
    "            # Get sequence of features for all assets\n",
    "            asset_sequences = []\n",
    "            for asset in self.assets:\n",
    "                asset_seq = aligned_features[asset].iloc[i-self.sequence_length:i].values\n",
    "                asset_sequences.append(asset_seq)\n",
    "                \n",
    "            # Stack all assets horizontally [sequence_length, features, n_assets]\n",
    "            combined_seq = np.stack(asset_sequences, axis=-1)\n",
    "            feature_matrix.append(combined_seq)\n",
    "            \n",
    "        feature_matrix = np.array(feature_matrix)\n",
    "        date_index = common_dates[self.sequence_length:]\n",
    "        \n",
    "        return feature_matrix, date_index\n",
    "    \n",
    "    def normalize_features(self, features: np.ndarray, fit: bool = True) -> np.ndarray:\n",
    "        \"\"\"Normalize features using StandardScaler\"\"\"\n",
    "        # Reshape for scaling: [samples * sequence_length * n_assets, features]\n",
    "        original_shape = features.shape\n",
    "        reshaped = features.reshape(-1, features.shape[2])\n",
    "        \n",
    "        if fit:\n",
    "            self.scalers['features'] = StandardScaler()\n",
    "            normalized = self.scalers['features'].fit_transform(reshaped)\n",
    "        else:\n",
    "            normalized = self.scalers['features'].transform(reshaped)\n",
    "            \n",
    "        # Reshape back to original\n",
    "        return normalized.reshape(original_shape)\n",
    "\n",
    "\n",
    "# ================== DATASET MODULE ==================\n",
    "\n",
    "class PortfolioDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for portfolio optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, features: np.ndarray, strategies: List[str], risk_tolerances: List[int]):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.n_samples = len(features)\n",
    "        self.n_assets = features.shape[3]\n",
    "        \n",
    "        # Strategy encoding\n",
    "        strategy_mapping = {\n",
    "            'max_sharpe': [1, 0, 0],\n",
    "            'min_volatility': [0, 1, 0],\n",
    "            'equal_risk_contribution': [0, 0, 1]\n",
    "        }\n",
    "        \n",
    "        # Create all combinations of features, strategies, and risk tolerances\n",
    "        self.samples = []\n",
    "        for i in range(self.n_samples):\n",
    "            for strategy in strategies:\n",
    "                for risk_tolerance in risk_tolerances:\n",
    "                    self.samples.append({\n",
    "                        'features': self.features[i],\n",
    "                        'strategy': torch.FloatTensor(strategy_mapping[strategy]),\n",
    "                        'risk_tolerance': torch.FloatTensor([risk_tolerance / 10.0])  # Normalize to [0,1]\n",
    "                    })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "# ================== MODEL MODULE ==================\n",
    "\n",
    "class PortfolioOptimizer(nn.Module):\n",
    "    \"\"\"Neural network for portfolio weight optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, n_features: int, n_assets: int, sequence_length: int):\n",
    "        super(PortfolioOptimizer, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_assets = n_assets\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Asset-wise LSTM encoder\n",
    "        self.asset_lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=64,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Asset feature encoder\n",
    "        self.asset_encoder = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Cross-asset attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=32,\n",
    "            num_heads=4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Strategy and risk encoding\n",
    "        self.strategy_encoder = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.risk_encoder = nn.Sequential(\n",
    "            nn.Linear(1, 8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Portfolio decoder\n",
    "        portfolio_input_size = 32 * n_assets + 16 + 8  # Asset features + strategy + risk\n",
    "        self.portfolio_decoder = nn.Sequential(\n",
    "            nn.Linear(portfolio_input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, n_assets),\n",
    "            nn.Softmax(dim=-1)  # Ensure weights sum to 1\n",
    "        )\n",
    "        \n",
    "    def forward(self, features, strategy, risk_tolerance):\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Process each asset through LSTM\n",
    "        asset_encodings = []\n",
    "        for i in range(self.n_assets):\n",
    "            asset_features = features[:, :, :, i]  # [batch, sequence, features]\n",
    "            lstm_out, _ = self.asset_lstm(asset_features)\n",
    "            # Take last hidden state\n",
    "            asset_encoding = lstm_out[:, -1, :]  # [batch, hidden_size]\n",
    "            asset_encoding = self.asset_encoder(asset_encoding)\n",
    "            asset_encodings.append(asset_encoding)\n",
    "        \n",
    "        # Stack asset encodings\n",
    "        asset_encodings = torch.stack(asset_encodings, dim=1)  # [batch, n_assets, encoding_dim]\n",
    "        \n",
    "        # Apply cross-asset attention\n",
    "        attended_features, _ = self.attention(\n",
    "            asset_encodings, asset_encodings, asset_encodings\n",
    "        )\n",
    "        \n",
    "        # Flatten asset features\n",
    "        flattened_assets = attended_features.reshape(batch_size, -1)\n",
    "        \n",
    "        # Encode strategy and risk\n",
    "        strategy_encoding = self.strategy_encoder(strategy)\n",
    "        risk_encoding = self.risk_encoder(risk_tolerance)\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_features = torch.cat([\n",
    "            flattened_assets,\n",
    "            strategy_encoding,\n",
    "            risk_encoding\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # Generate portfolio weights\n",
    "        weights = self.portfolio_decoder(combined_features)\n",
    "        \n",
    "        return weights\n",
    "\n",
    "\n",
    "# ================== LOSS FUNCTION MODULE ==================\n",
    "\n",
    "class PortfolioLoss(nn.Module):\n",
    "    \"\"\"Portfolio optimization loss function\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PortfolioLoss, self).__init__()\n",
    "        \n",
    "    def calculate_portfolio_metrics(self, weights, returns, covariance_matrix):\n",
    "        \"\"\"Calculate portfolio return, volatility, and risk contributions\"\"\"\n",
    "        # Portfolio return (assuming equal weighting for historical returns)\n",
    "        portfolio_returns = torch.sum(weights.unsqueeze(1) * returns, dim=-1)\n",
    "        mean_return = torch.mean(portfolio_returns, dim=1)\n",
    "        \n",
    "        # Portfolio volatility\n",
    "        portfolio_variance = torch.sum(\n",
    "            weights.unsqueeze(1) * (covariance_matrix * weights.unsqueeze(2)), \n",
    "            dim=[-1, -2]\n",
    "        )\n",
    "        portfolio_vol = torch.sqrt(portfolio_variance + 1e-8)\n",
    "        \n",
    "        # Risk contributions for equal risk contribution strategy\n",
    "        marginal_risk = torch.sum(covariance_matrix * weights.unsqueeze(2), dim=-1)\n",
    "        risk_contributions = weights * marginal_risk / (portfolio_vol.unsqueeze(1) + 1e-8)\n",
    "        \n",
    "        return mean_return, portfolio_vol, risk_contributions\n",
    "    \n",
    "    def forward(self, weights, strategy, risk_tolerance, returns, covariance_matrix):\n",
    "        # Calculate portfolio metrics\n",
    "        portfolio_return, portfolio_vol, risk_contributions = self.calculate_portfolio_metrics(\n",
    "            weights, returns, covariance_matrix\n",
    "        )\n",
    "        \n",
    "        # Initialize loss\n",
    "        loss = torch.zeros_like(portfolio_return)\n",
    "        \n",
    "        # Max Sharpe strategy\n",
    "        max_sharpe_mask = strategy[:, 0] > 0.5\n",
    "        if max_sharpe_mask.sum() > 0:\n",
    "            sharpe_ratio = portfolio_return[max_sharpe_mask] / (portfolio_vol[max_sharpe_mask] + 1e-8)\n",
    "            loss[max_sharpe_mask] = -sharpe_ratio  # Negative because we want to maximize\n",
    "        \n",
    "        # Min Volatility strategy\n",
    "        min_vol_mask = strategy[:, 1] > 0.5\n",
    "        if min_vol_mask.sum() > 0:\n",
    "            loss[min_vol_mask] = portfolio_vol[min_vol_mask]\n",
    "        \n",
    "        # Equal Risk Contribution strategy\n",
    "        erc_mask = strategy[:, 2] > 0.5\n",
    "        if erc_mask.sum() > 0:\n",
    "            n_assets = weights.shape[1]\n",
    "            target_contribution = 1.0 / n_assets\n",
    "            contribution_diff = risk_contributions[erc_mask] - target_contribution\n",
    "            loss[erc_mask] = torch.sum(contribution_diff ** 2, dim=1)\n",
    "        \n",
    "        # Risk tolerance adjustment\n",
    "        risk_penalty = (1 - risk_tolerance.squeeze()) * portfolio_vol\n",
    "        loss += risk_penalty\n",
    "        \n",
    "        return torch.mean(loss)\n",
    "\n",
    "\n",
    "# ================== TRAINING MODULE ==================\n",
    "\n",
    "class PortfolioTrainer:\n",
    "    \"\"\"Trainer for portfolio optimization model\"\"\"\n",
    "    \n",
    "    def __init__(self, model: PortfolioOptimizer, device: str = 'cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.criterion = PortfolioLoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=10\n",
    "        )\n",
    "        \n",
    "    def prepare_batch_data(self, batch, historical_returns, covariance_matrices):\n",
    "        \"\"\"Prepare batch data for training\"\"\"\n",
    "        batch_size = len(batch['features'])\n",
    "        \n",
    "        # Get historical returns and covariance for this batch\n",
    "        # This is a simplified version - in practice, you'd want to calculate\n",
    "        # returns and covariance from the actual feature data\n",
    "        returns = historical_returns[:batch_size]\n",
    "        cov_matrix = covariance_matrices[:batch_size]\n",
    "        \n",
    "        return (\n",
    "            batch['features'].to(self.device),\n",
    "            batch['strategy'].to(self.device),\n",
    "            batch['risk_tolerance'].to(self.device),\n",
    "            returns.to(self.device),\n",
    "            cov_matrix.to(self.device)\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self, dataloader, historical_returns, covariance_matrices):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            features, strategy, risk_tolerance, returns, cov_matrix = self.prepare_batch_data(\n",
    "                batch, historical_returns, covariance_matrices\n",
    "            )\n",
    "            \n",
    "            # Forward pass\n",
    "            weights = self.model(features, strategy, risk_tolerance)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.criterion(weights, strategy, risk_tolerance, returns, cov_matrix)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(dataloader)\n",
    "    \n",
    "    def validate_epoch(self, dataloader, historical_returns, covariance_matrices):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                features, strategy, risk_tolerance, returns, cov_matrix = self.prepare_batch_data(\n",
    "                    batch, historical_returns, covariance_matrices\n",
    "                )\n",
    "                \n",
    "                weights = self.model(features, strategy, risk_tolerance)\n",
    "                loss = self.criterion(weights, strategy, risk_tolerance, returns, cov_matrix)\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(dataloader)\n",
    "    \n",
    "    def train(self, train_loader, val_loader, historical_returns, covariance_matrices, epochs: int = 100):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            train_loss = self.train_epoch(train_loader, historical_returns, covariance_matrices)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss = self.validate_epoch(val_loader, historical_returns, covariance_matrices)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.model.state_dict(), 'best_portfolio_model.pth')\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Load best model\n",
    "        self.model.load_state_dict(torch.load('best_portfolio_model.pth'))\n",
    "        \n",
    "        return train_losses, val_losses\n",
    "\n",
    "\n",
    "# ================== MAIN EXECUTION MODULE ==================\n",
    "\n",
    "class PortfolioOptimizationPipeline:\n",
    "    \"\"\"Complete pipeline for portfolio optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, assets: List[str], fetch_function):\n",
    "        self.assets = assets\n",
    "        self.fetch_function = fetch_function\n",
    "        self.processor = None\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "        \n",
    "    def prepare_data(self, sequence_length: int = 60):\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "        print(\"Preparing data...\")\n",
    "        self.processor = PortfolioDataProcessor(self.assets, self.fetch_function, sequence_length)\n",
    "        \n",
    "        # Load and process data\n",
    "        features, dates = self.processor.load_and_process_data()\n",
    "        \n",
    "        # Normalize features\n",
    "        features = self.processor.normalize_features(features, fit=True)\n",
    "        \n",
    "        # Calculate historical returns and covariance matrices\n",
    "        # This is simplified - in practice, you'd calculate these from the actual price data\n",
    "        n_samples, seq_len, n_features, n_assets = features.shape\n",
    "        historical_returns = torch.randn(n_samples, seq_len, n_assets)  # Placeholder\n",
    "        covariance_matrices = torch.eye(n_assets).unsqueeze(0).repeat(n_samples, 1, 1)  # Placeholder\n",
    "        \n",
    "        return features, historical_returns, covariance_matrices, dates\n",
    "    \n",
    "    def create_datasets(self, features, strategies=None, risk_tolerances=None, test_size=0.2):\n",
    "        \"\"\"Create train/validation datasets\"\"\"\n",
    "        if strategies is None:\n",
    "            strategies = ['max_sharpe', 'min_volatility', 'equal_risk_contribution']\n",
    "        if risk_tolerances is None:\n",
    "            risk_tolerances = list(range(1, 11))\n",
    "        \n",
    "        # Split data\n",
    "        train_features, val_features = train_test_split(\n",
    "            features, test_size=test_size, random_state=42, shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = PortfolioDataset(train_features, strategies, risk_tolerances)\n",
    "        val_dataset = PortfolioDataset(val_features, strategies, risk_tolerances)\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def train_model(self, train_dataset, val_dataset, historical_returns, covariance_matrices, \n",
    "                   batch_size=32, epochs=100):\n",
    "        \"\"\"Train the portfolio optimization model\"\"\"\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Initialize model\n",
    "        n_features = train_dataset.samples[0]['features'].shape[1]\n",
    "        sequence_length = train_dataset.samples[0]['features'].shape[0]\n",
    "        n_assets = len(self.assets)\n",
    "        \n",
    "        self.model = PortfolioOptimizer(n_features, n_assets, sequence_length)\n",
    "        \n",
    "        # Initialize trainer\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.trainer = PortfolioTrainer(self.model, device)\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Training model...\")\n",
    "        train_losses, val_losses = self.trainer.train(\n",
    "            train_loader, val_loader, historical_returns, covariance_matrices, epochs\n",
    "        )\n",
    "        \n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def predict_weights(self, features, strategy: str, risk_tolerance: int):\n",
    "        \"\"\"Predict optimal portfolio weights\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet!\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Prepare strategy encoding\n",
    "        strategy_mapping = {\n",
    "            'max_sharpe': [1, 0, 0],\n",
    "            'min_volatility': [0, 1, 0],\n",
    "            'equal_risk_contribution': [0, 0, 1]\n",
    "        }\n",
    "        \n",
    "        strategy_tensor = torch.FloatTensor(strategy_mapping[strategy]).unsqueeze(0)\n",
    "        risk_tensor = torch.FloatTensor([risk_tolerance / 10.0]).unsqueeze(0)\n",
    "        features_tensor = torch.FloatTensor(features).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            weights = self.model(features_tensor, strategy_tensor, risk_tensor)\n",
    "            \n",
    "        return weights.squeeze().numpy()\n",
    "    \n",
    "    def run_complete_pipeline(self):\n",
    "        \"\"\"Run the complete pipeline\"\"\"\n",
    "        try:\n",
    "            # 1. Prepare data\n",
    "            features, historical_returns, covariance_matrices, dates = self.prepare_data()\n",
    "            \n",
    "            # 2. Create datasets\n",
    "            train_dataset, val_dataset = self.create_datasets(features)\n",
    "            \n",
    "            # 3. Train model\n",
    "            train_losses, val_losses = self.train_model(\n",
    "                train_dataset, val_dataset, historical_returns, covariance_matrices, epochs = 1\n",
    "            )\n",
    "            \n",
    "            # 4. Plot training curves\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(train_losses, label='Training Loss')\n",
    "            plt.plot(val_losses, label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.title('Training Curves')\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            # Example prediction\n",
    "            sample_features = features[-1]  # Last time step\n",
    "            weights = self.predict_weights(sample_features, 'max_sharpe', 5)\n",
    "            \n",
    "            plt.bar(self.assets, weights)\n",
    "            plt.xlabel('Assets')\n",
    "            plt.ylabel('Weights')\n",
    "            plt.title('Sample Portfolio Weights (Max Sharpe, Risk=5)')\n",
    "            plt.xticks(rotation=45)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"Pipeline completed successfully!\")\n",
    "            print(f\"Sample prediction - Asset weights: {dict(zip(self.assets, weights))}\")\n",
    "            \n",
    "            return self.model, train_losses, val_losses\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in pipeline: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# ================== EXAMPLE USAGE ==================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage - replace with your actual fetch function\n",
    "    # def dummy_fetch(ticker):\n",
    "    #     \"\"\"Dummy fetch function for demonstration\"\"\"\n",
    "    #     dates = pd.date_range('2019-01-01', '2024-05-16', freq='D')\n",
    "    #     np.random.seed(hash(ticker) % 2**32)  # Consistent random data per ticker\n",
    "        \n",
    "    #     n_days = len(dates)\n",
    "    #     price = 100 * np.exp(np.cumsum(np.random.normal(0.0001, 0.02, n_days)))\n",
    "        \n",
    "    #     df = pd.DataFrame({\n",
    "    #         'Date': dates,\n",
    "    #         'Open': price * np.random.uniform(0.98, 1.02, n_days),\n",
    "    #         'High': price * np.random.uniform(1.00, 1.05, n_days),\n",
    "    #         'Low': price * np.random.uniform(0.95, 1.00, n_days),\n",
    "    #         'Close': price,\n",
    "    #         'Volume': np.random.randint(1000000, 50000000, n_days),\n",
    "    #         'Dividends': np.zeros(n_days),\n",
    "    #         'Stock Splits': np.zeros(n_days)\n",
    "    #     })\n",
    "    #     df.set_index('Date', inplace=True)\n",
    "    #     return df\n",
    "    \n",
    "    # Define portfolio assets\n",
    "    assets = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'SPY']\n",
    "    \n",
    "    # Run pipeline\n",
    "    pipeline = PortfolioOptimizationPipeline(assets, fetch_and_store_historical)\n",
    "    model, train_losses, val_losses = pipeline.run_complete_pipeline()\n",
    "    \n",
    "    # Example predictions for different strategies\n",
    "    sample_features = pipeline.processor.normalize_features(\n",
    "        features=np.random.randn(1, 60, len(pipeline.processor.feature_names), len(assets)),\n",
    "        fit=False\n",
    "    )[0]\n",
    "    \n",
    "    print(\"\\nExample Predictions:\")\n",
    "    for strategy in ['max_sharpe', 'min_volatility', 'equal_risk_contribution']:\n",
    "        for risk in [3, 5, 8]:\n",
    "            weights = pipeline.predict_weights(sample_features, strategy, risk)\n",
    "            print(f\"{strategy.replace('_', ' ').title()} (Risk={risk}): {dict(zip(assets, np.round(weights, 3)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a1bc8d0-3b8e-4cec-a612-8d7f931e6102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Fetched data for AAPL\n",
      "Fetched data for MSFT\n",
      "Fetched data for AMZN\n",
      "Fetched data for GOOGL\n",
      "Fetched data for META\n",
      "Fetched data for TSLA\n",
      "Fetched data for BRK-B\n",
      "Fetched data for JPM\n",
      "Fetched data for JNJ\n",
      "Fetched data for V\n",
      "Training model...\n",
      "Epoch 1/1 - Train Loss: 0.007841, Val Loss: 0.007432\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54946/2961482869.py:685: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load('best_portfolio_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.009925\n",
      "Running backtest...\n",
      "Comparing strategies...\n",
      "Backtesting Maximum Sharpe Ratio...\n",
      "Backtesting Minimum Volatility...\n",
      "Backtesting Equal Risk Contribution...\n",
      "Backtesting High Risk (Max Sharpe)...\n",
      "Backtesting Low Risk (Min Vol)...\n",
      "\n",
      "Performance Summary:\n",
      "                        Annual Return Annual Volatility Sharpe Ratio  \\\n",
      "Maximum Sharpe Ratio            6.18%            35.18%         0.18   \n",
      "Minimum Volatility              5.95%            34.20%         0.17   \n",
      "Equal Risk Contribution         3.66%            31.60%         0.12   \n",
      "High Risk (Max Sharpe)          6.27%            35.24%         0.18   \n",
      "Low Risk (Min Vol)              5.80%            34.11%         0.17   \n",
      "\n",
      "                        Max Drawdown  \n",
      "Maximum Sharpe Ratio          16.11%  \n",
      "Minimum Volatility            15.89%  \n",
      "Equal Risk Contribution       15.23%  \n",
      "High Risk (Max Sharpe)        16.12%  \n",
      "Low Risk (Min Vol)            15.88%  \n",
      "\n",
      "Optimizing portfolio for subset: ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META']\n",
      "\n",
      "Optimized Weights:\n",
      "       Weight\n",
      "AAPL   24.93%\n",
      "MSFT   20.03%\n",
      "AMZN   15.37%\n",
      "GOOGL  19.68%\n",
      "META   19.99%\n",
      "\n",
      "Expected Portfolio Metrics:\n",
      "Expected Annual Return: 15.08%\n",
      "Expected Annual Volatility: 26.84%\n",
      "Expected Sharpe Ratio: 0.56\n",
      "\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "\n",
    "# Define the top 10 stocks to use as our universal set\n",
    "TOP_STOCKS = [\n",
    "    'AAPL',  # Apple\n",
    "    'MSFT',  # Microsoft\n",
    "    'AMZN',  # Amazon\n",
    "    'GOOGL', # Alphabet\n",
    "    'META',  # Meta Platforms\n",
    "    'TSLA',  # Tesla\n",
    "    'BRK-B', # Berkshire Hathaway\n",
    "    'JPM',   # JPMorgan Chase\n",
    "    'JNJ',   # Johnson & Johnson\n",
    "    'V'      # Visa\n",
    "]\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Module for processing historical price data\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_returns(prices_df: pd.DataFrame, period: int = 1) -> pd.DataFrame:\n",
    "        \"\"\"Calculate returns from price data using closing prices\"\"\"\n",
    "        return prices_df['Close'].pct_change(period).dropna()\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_features(prices_df: pd.DataFrame, window: int = 20) -> pd.DataFrame:\n",
    "        \"\"\"Extract features from price data\"\"\"\n",
    "        # Using closing prices for calculations\n",
    "        close_prices = prices_df['Close'].values\n",
    "        \n",
    "        features = pd.DataFrame()\n",
    "        # Returns over different periods\n",
    "        features['returns_1d'] = np.append(np.nan, np.diff(close_prices) / close_prices[:-1])\n",
    "        \n",
    "        # Volatility features\n",
    "        features['volatility_20d'] = features['returns_1d'].rolling(window=window).std()\n",
    "        \n",
    "        # Momentum features\n",
    "        features['momentum_20d'] = close_prices / close_prices[-window] - 1 if len(close_prices) >= window else np.nan\n",
    "        \n",
    "        # Volume features if available\n",
    "        if 'Volume' in prices_df.columns:\n",
    "            features['volume_ratio'] = prices_df['Volume'] / prices_df['Volume'].rolling(window=window).mean()\n",
    "        \n",
    "        # Additional technical indicators could be added here\n",
    "        \n",
    "        return features.dropna()\n",
    "    \n",
    "    @staticmethod\n",
    "    def align_data_for_multiple_assets(assets_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Ensure all assets data have the same date range\"\"\"\n",
    "        # Find common date range\n",
    "        common_dates = None\n",
    "        \n",
    "        for ticker, df in assets_data.items():\n",
    "            if common_dates is None:\n",
    "                common_dates = set(df.index)\n",
    "            else:\n",
    "                common_dates = common_dates.intersection(set(df.index))\n",
    "        \n",
    "        common_dates = sorted(list(common_dates))\n",
    "        \n",
    "        # Filter all dataframes to common date range\n",
    "        aligned_data = {}\n",
    "        for ticker, df in assets_data.items():\n",
    "            aligned_data[ticker] = df.loc[common_dates]\n",
    "            \n",
    "        return aligned_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_features(features_dict: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Normalize features for all assets\"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        normalized_features = {}\n",
    "        \n",
    "        # Combine all features to fit the scaler\n",
    "        all_features = pd.concat([df for df in features_dict.values()], axis=0)\n",
    "        scaler.fit(all_features.values)\n",
    "        \n",
    "        # Apply the scaler to each asset's features\n",
    "        for ticker, features in features_dict.items():\n",
    "            normalized_features[ticker] = pd.DataFrame(\n",
    "                scaler.transform(features.values),\n",
    "                index=features.index,\n",
    "                columns=features.columns\n",
    "            )\n",
    "            \n",
    "        return normalized_features\n",
    "\n",
    "\n",
    "class PortfolioDataset(Dataset):\n",
    "    \"\"\"Dataset for portfolio optimization model training and evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 returns_dict: Dict[str, pd.DataFrame],\n",
    "                 features_dict: Dict[str, pd.DataFrame],\n",
    "                 window_size: int = 252,\n",
    "                 num_samples: int = 1000,\n",
    "                 tickers: List[str] = TOP_STOCKS,\n",
    "                 training: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize dataset\n",
    "        \n",
    "        Args:\n",
    "            returns_dict: Dictionary of returns dataframes for each ticker\n",
    "            features_dict: Dictionary of features dataframes for each ticker\n",
    "            window_size: Number of days to use for historical window\n",
    "            num_samples: Number of samples to generate for training\n",
    "            tickers: List of tickers to include in the universe\n",
    "            training: Whether this is for training (random sampling) or testing\n",
    "        \"\"\"\n",
    "        self.returns_dict = returns_dict\n",
    "        self.features_dict = features_dict\n",
    "        self.window_size = window_size\n",
    "        self.num_samples = num_samples\n",
    "        self.tickers = tickers\n",
    "        self.num_assets = len(tickers)\n",
    "        self.training = training\n",
    "        \n",
    "        # Ensure all tickers have data\n",
    "        valid_tickers = []\n",
    "        for ticker in self.tickers:\n",
    "            if ticker in self.returns_dict and ticker in self.features_dict:\n",
    "                valid_tickers.append(ticker)\n",
    "        \n",
    "        self.tickers = valid_tickers\n",
    "        self.num_assets = len(valid_tickers)\n",
    "        \n",
    "        # Find common dates across all tickers for both returns and features\n",
    "        common_dates = None\n",
    "        for ticker in self.tickers:\n",
    "            returns_dates = set(self.returns_dict[ticker].index)\n",
    "            features_dates = set(self.features_dict[ticker].index)\n",
    "            ticker_dates = returns_dates.intersection(features_dates)\n",
    "            \n",
    "            if common_dates is None:\n",
    "                common_dates = ticker_dates\n",
    "            else:\n",
    "                common_dates = common_dates.intersection(ticker_dates)\n",
    "        \n",
    "        # Convert to list and sort\n",
    "        self.aligned_dates = sorted(list(common_dates))\n",
    "        \n",
    "        # Ensure there's enough data for the window size\n",
    "        if len(self.aligned_dates) <= window_size:\n",
    "            raise ValueError(f\"Not enough data for window size {window_size}. Only {len(self.aligned_dates)} dates available.\")\n",
    "        \n",
    "        self.valid_start_indices = list(range(len(self.aligned_dates) - window_size))\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.training:\n",
    "            return self.num_samples\n",
    "        else:\n",
    "            return len(self.valid_start_indices)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.training:\n",
    "            # Random starting point for training\n",
    "            start_idx = np.random.choice(self.valid_start_indices)\n",
    "            \n",
    "            # Random subset of assets (at least 2 assets)\n",
    "            num_active = np.random.randint(2, self.num_assets + 1)\n",
    "            active_indices = np.random.choice(self.num_assets, num_active, replace=False)\n",
    "            \n",
    "            # Random optimization strategy (0: Max Sharpe, 1: Min Vol, 2: ERC)\n",
    "            strategy = np.random.randint(0, 3)\n",
    "            \n",
    "            # Random risk tolerance\n",
    "            risk_tolerance = np.random.randint(1, 11)\n",
    "            \n",
    "        else:\n",
    "            # Sequential for testing\n",
    "            start_idx = self.valid_start_indices[idx]\n",
    "            \n",
    "            # Use all assets for testing\n",
    "            active_indices = np.arange(self.num_assets)\n",
    "            \n",
    "            # Fixed parameters for consistent testing\n",
    "            strategy = 0  # Max Sharpe\n",
    "            risk_tolerance = 5  # Moderate risk\n",
    "        \n",
    "        # Get the corresponding dates for this window\n",
    "        end_idx = start_idx + self.window_size\n",
    "        window_dates = self.aligned_dates[start_idx:end_idx]\n",
    "        \n",
    "        # Create mask for active assets\n",
    "        mask = np.zeros(self.num_assets)\n",
    "        mask[active_indices] = 1\n",
    "        \n",
    "        # Prepare strategy one-hot encoding\n",
    "        strategy_onehot = np.zeros(3)\n",
    "        strategy_onehot[strategy] = 1\n",
    "        \n",
    "        # Collect returns and features for all assets in the window\n",
    "        returns_window = np.zeros((self.window_size, self.num_assets))\n",
    "        features_window = np.zeros((self.window_size, self.num_assets, 4))  # Assuming 4 features per asset\n",
    "        \n",
    "        for i, ticker in enumerate(self.tickers):\n",
    "            # Extract returns for this window\n",
    "            ticker_returns = self.returns_dict[ticker].loc[window_dates].values\n",
    "            returns_window[:, i] = ticker_returns\n",
    "            \n",
    "            # Extract features for this window\n",
    "            ticker_features = self.features_dict[ticker].loc[window_dates].values\n",
    "            # Handle case where features might have different dimensions\n",
    "            if ticker_features.ndim == 1:\n",
    "                features_window[:, i, 0] = ticker_features\n",
    "            else:\n",
    "                # Handle case where the feature matrix may have fewer than 4 features\n",
    "                n_features = min(ticker_features.shape[1], 4)\n",
    "                features_window[:, i, :n_features] = ticker_features[:, :n_features]\n",
    "        \n",
    "        # Calculate target weights based on strategy and active assets\n",
    "        active_returns = returns_window[:, active_indices]\n",
    "        target_weights = self._calculate_target_weights(\n",
    "            active_returns,\n",
    "            strategy,\n",
    "            risk_tolerance,\n",
    "            active_indices\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'returns': torch.tensor(returns_window, dtype=torch.float32),\n",
    "            'features': torch.tensor(features_window, dtype=torch.float32),\n",
    "            'mask': torch.tensor(mask, dtype=torch.float32),\n",
    "            'strategy': torch.tensor(strategy_onehot, dtype=torch.float32),\n",
    "            'risk_tolerance': torch.tensor([risk_tolerance/10.0], dtype=torch.float32),\n",
    "            'target_weights': torch.tensor(target_weights, dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "    def _calculate_target_weights(self, \n",
    "                                 returns: np.ndarray,\n",
    "                                 strategy: int,\n",
    "                                 risk_tolerance: int,\n",
    "                                 active_indices: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate target weights based on optimization strategy\n",
    "        \n",
    "        Args:\n",
    "            returns: Returns array for active assets [window_size, num_active]\n",
    "            strategy: Strategy index (0: Max Sharpe, 1: Min Vol, 2: ERC)\n",
    "            risk_tolerance: Risk tolerance level (1-10)\n",
    "            active_indices: Indices of active assets\n",
    "            \n",
    "        Returns:\n",
    "            Array of target weights [num_assets]\n",
    "        \"\"\"\n",
    "        num_active = len(active_indices)\n",
    "        full_weights = np.zeros(self.num_assets)\n",
    "        \n",
    "        # Skip the first few returns which might be NaN\n",
    "        valid_returns = returns[~np.isnan(returns).any(axis=1)]\n",
    "        \n",
    "        # If no valid returns, use equal weights\n",
    "        if len(valid_returns) < 2:\n",
    "            active_weights = np.ones(num_active) / num_active\n",
    "            full_weights[active_indices] = active_weights\n",
    "            return full_weights\n",
    "        \n",
    "        if strategy == 0:  # Maximum Sharpe Ratio\n",
    "            # Simple estimation of Sharpe-optimal weights\n",
    "            mean_returns = np.mean(valid_returns, axis=0)\n",
    "            cov_matrix = np.cov(valid_returns.T) if valid_returns.shape[0] > 1 else np.eye(valid_returns.shape[1])\n",
    "            \n",
    "            # Handle potential numerical issues\n",
    "            if np.any(np.diag(cov_matrix) <= 0):\n",
    "                active_weights = np.ones(num_active) / num_active\n",
    "            else:\n",
    "                # Simplified approach, not true max Sharpe optimization\n",
    "                vol = np.sqrt(np.diag(cov_matrix))\n",
    "                sharpe = mean_returns / vol\n",
    "                \n",
    "                # Adjust based on risk tolerance\n",
    "                sharpe_adj = sharpe * (risk_tolerance / 5.0)\n",
    "                \n",
    "                if np.sum(sharpe_adj > 0) > 0:\n",
    "                    active_weights = np.maximum(sharpe_adj, 0)\n",
    "                    if np.sum(active_weights) > 0:\n",
    "                        active_weights = active_weights / np.sum(active_weights)\n",
    "                    else:\n",
    "                        active_weights = np.ones(num_active) / num_active\n",
    "                else:\n",
    "                    active_weights = np.ones(num_active) / num_active\n",
    "                    \n",
    "        elif strategy == 1:  # Minimum Volatility\n",
    "            # Simple estimation of min-vol weights\n",
    "            if valid_returns.shape[0] > 1:\n",
    "                cov_matrix = np.cov(valid_returns.T)\n",
    "            else:\n",
    "                # If not enough data, use identity matrix\n",
    "                cov_matrix = np.eye(valid_returns.shape[1])\n",
    "            \n",
    "            # Use inverse volatility weighting as an approximation\n",
    "            vol = np.sqrt(np.diag(cov_matrix))\n",
    "            if np.any(vol <= 0):\n",
    "                active_weights = np.ones(num_active) / num_active\n",
    "            else:\n",
    "                inv_vol = 1.0 / vol\n",
    "                active_weights = inv_vol / np.sum(inv_vol)\n",
    "                \n",
    "            # Adjust towards equal weights for higher risk tolerance\n",
    "            equal_weights = np.ones(num_active) / num_active\n",
    "            risk_factor = risk_tolerance / 10.0\n",
    "            active_weights = (1 - risk_factor) * active_weights + risk_factor * equal_weights\n",
    "                \n",
    "        else:  # Equal Risk Contribution\n",
    "            if valid_returns.shape[0] > 1:\n",
    "                cov_matrix = np.cov(valid_returns.T)\n",
    "            else:\n",
    "                # If not enough data, use identity matrix\n",
    "                cov_matrix = np.eye(valid_returns.shape[1])\n",
    "                \n",
    "            active_weights = self._equal_risk_contribution(cov_matrix)\n",
    "            \n",
    "            # For higher risk tolerance, allow more concentration\n",
    "            if risk_tolerance > 5:\n",
    "                mean_returns = np.mean(valid_returns, axis=0)\n",
    "                \n",
    "                # Find the assets with above-average returns\n",
    "                above_avg = mean_returns > np.mean(mean_returns)\n",
    "                \n",
    "                # Skew weights towards better performers based on risk tolerance\n",
    "                skew_factor = (risk_tolerance - 5) / 5.0\n",
    "                skew = 1.0 + skew_factor * above_avg\n",
    "                \n",
    "                active_weights = active_weights * skew\n",
    "                if np.sum(active_weights) > 0:\n",
    "                    active_weights = active_weights / np.sum(active_weights)\n",
    "                else:\n",
    "                    active_weights = np.ones(num_active) / num_active\n",
    "        \n",
    "        # Assign weights to active assets in the full portfolio\n",
    "        full_weights[active_indices] = active_weights\n",
    "        return full_weights\n",
    "    \n",
    "    def _equal_risk_contribution(self, cov_matrix: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate weights for equal risk contribution\n",
    "        \n",
    "        Args:\n",
    "            cov_matrix: Covariance matrix [num_active, num_active]\n",
    "            \n",
    "        Returns:\n",
    "            Array of weights [num_active]\n",
    "        \"\"\"\n",
    "        n = cov_matrix.shape[0]\n",
    "        weights = np.ones(n) / n\n",
    "        \n",
    "        # Handle case where covariance matrix is all zeros\n",
    "        if np.all(cov_matrix == 0):\n",
    "            return weights\n",
    "        \n",
    "        # Simple optimization approach for ERC\n",
    "        for _ in range(100):\n",
    "            # Calculate portfolio volatility\n",
    "            portfolio_vol = np.sqrt(weights.T @ cov_matrix @ weights)\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            if portfolio_vol <= 1e-10:\n",
    "                return np.ones(n) / n\n",
    "            \n",
    "            # Calculate marginal risk contribution\n",
    "            marginal_contrib = (cov_matrix @ weights) / portfolio_vol\n",
    "            \n",
    "            # Calculate risk contribution\n",
    "            risk_contrib = weights * marginal_contrib\n",
    "            \n",
    "            # Target equal risk contribution\n",
    "            target_risk = np.sum(risk_contrib) / n\n",
    "            \n",
    "            # Update weights - move towards equal risk contribution\n",
    "            # Avoid division by zero\n",
    "            adjustment = np.ones(n)\n",
    "            for i in range(n):\n",
    "                if risk_contrib[i] > 1e-10:\n",
    "                    adjustment[i] = target_risk / risk_contrib[i]\n",
    "            \n",
    "            weights = weights * adjustment\n",
    "            weights = weights / np.sum(weights)\n",
    "        \n",
    "        return weights\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Self-attention block for modeling asset interactions\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, num_heads: int = 4):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class AssetEncoder(nn.Module):\n",
    "    \"\"\"Encodes historical asset data into embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim: int, \n",
    "                 hidden_dim: int, \n",
    "                 window_size: int,\n",
    "                 num_layers: int = 1):\n",
    "        super(AssetEncoder, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_dim, kernel_size=5, padding=2),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Calculate output size after pooling\n",
    "        self.pooled_size = window_size // 4\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.output_proj = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, window_size, input_dim]\n",
    "        batch_size, window_size, input_dim = x.size()\n",
    "        \n",
    "        # Prepare for Conv1d [batch_size, input_dim, window_size]\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Extract features\n",
    "        x = self.feature_extractor(x)  # [batch_size, hidden_dim, pooled_size]\n",
    "        \n",
    "        # Prepare for LSTM [batch_size, pooled_size, hidden_dim]\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Process with LSTM\n",
    "        lstm_out, _ = self.lstm(x)  # [batch_size, pooled_size, hidden_dim*2]\n",
    "        \n",
    "        # Use the final output of the LSTM\n",
    "        x = lstm_out[:, -1, :]  # [batch_size, hidden_dim*2]\n",
    "        \n",
    "        # Project to final embedding dimension\n",
    "        x = self.output_proj(x)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class PortfolioOptimizationModel(nn.Module):\n",
    "    \"\"\"End-to-end portfolio optimization model\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_assets: int = 10, \n",
    "                 window_size: int = 252, \n",
    "                 hidden_dim: int = 64,\n",
    "                 feature_dim: int = 4,\n",
    "                 num_strategies: int = 3):\n",
    "        super(PortfolioOptimizationModel, self).__init__()\n",
    "        \n",
    "        self.num_assets = num_assets\n",
    "        \n",
    "        # Asset-specific encoders\n",
    "        self.return_encoder = AssetEncoder(1, hidden_dim, window_size)\n",
    "        self.feature_encoder = AssetEncoder(feature_dim, hidden_dim, window_size)\n",
    "        \n",
    "        # Asset embedding combination\n",
    "        self.asset_combiner = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        # Cross-asset attention\n",
    "        self.cross_asset_attention = AttentionBlock(hidden_dim)\n",
    "        \n",
    "        # Strategy-specific processing\n",
    "        self.strategy_embedding = nn.Embedding(num_strategies, hidden_dim)\n",
    "        \n",
    "        # Risk tolerance processing\n",
    "        self.risk_embedding = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Combined processing\n",
    "        self.combined_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Weight generator\n",
    "        self.weight_generator = nn.Linear(hidden_dim, num_assets)\n",
    "        \n",
    "    def forward(self, \n",
    "                returns: torch.Tensor,\n",
    "                features: torch.Tensor,\n",
    "                mask: torch.Tensor, \n",
    "                strategy: torch.Tensor, \n",
    "                risk_tolerance: torch.Tensor):\n",
    "        batch_size = returns.size(0)\n",
    "        \n",
    "        # Process each asset individually\n",
    "        asset_embeddings = []\n",
    "        for i in range(self.num_assets):\n",
    "            # Extract data for this asset\n",
    "            asset_returns = returns[:, :, i:i+1]  # [batch_size, window_size, 1]\n",
    "            asset_features = features[:, :, i, :]  # [batch_size, window_size, feature_dim]\n",
    "            \n",
    "            # Encode asset data\n",
    "            return_embedding = self.return_encoder(asset_returns)  # [batch_size, hidden_dim]\n",
    "            feature_embedding = self.feature_encoder(asset_features)  # [batch_size, hidden_dim]\n",
    "            \n",
    "            # Combine embeddings\n",
    "            asset_embedding = torch.cat([return_embedding, feature_embedding], dim=1)  # [batch_size, hidden_dim*2]\n",
    "            asset_embedding = self.asset_combiner(asset_embedding)  # [batch_size, hidden_dim]\n",
    "            \n",
    "            asset_embeddings.append(asset_embedding)\n",
    "        \n",
    "        # Stack asset embeddings [batch_size, num_assets, hidden_dim]\n",
    "        asset_embeddings = torch.stack(asset_embeddings, dim=1)\n",
    "        \n",
    "        # Apply cross-asset attention\n",
    "        asset_embeddings = self.cross_asset_attention(asset_embeddings)\n",
    "        \n",
    "        # Process strategy - convert one-hot to index\n",
    "        strategy_idx = torch.argmax(strategy, dim=1)\n",
    "        strategy_embedding = self.strategy_embedding(strategy_idx)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Process risk tolerance\n",
    "        risk_embedding = self.risk_embedding(risk_tolerance)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Combine strategy and risk embeddings\n",
    "        context_embedding = torch.cat([\n",
    "            strategy_embedding,\n",
    "            risk_embedding\n",
    "        ], dim=1)  # [batch_size, hidden_dim*2]\n",
    "        \n",
    "        # Pool asset embeddings with attention to mask\n",
    "        # Apply mask to asset embeddings\n",
    "        masked_embeddings = asset_embeddings * mask.unsqueeze(-1)\n",
    "        \n",
    "        # Sum pooling of masked embeddings\n",
    "        pooled_embedding = masked_embeddings.sum(dim=1) / (mask.sum(dim=1, keepdim=True) + 1e-10)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Combine pooled asset embedding with context\n",
    "        combined = torch.cat([\n",
    "            pooled_embedding,\n",
    "            context_embedding\n",
    "        ], dim=1)  # [batch_size, hidden_dim*3]\n",
    "        \n",
    "        # Process combined features\n",
    "        hidden = self.combined_layer(combined)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Generate unnormalized weights\n",
    "        raw_weights = self.weight_generator(hidden)  # [batch_size, num_assets]\n",
    "        \n",
    "        # Apply mask and softmax to get normalized weights for active assets only\n",
    "        masked_weights = raw_weights * mask - 1e10 * (1 - mask)\n",
    "        normalized_weights = torch.softmax(masked_weights, dim=1)\n",
    "        \n",
    "        return normalized_weights\n",
    "\n",
    "\n",
    "class PortfolioOptimizer:\n",
    "    \"\"\"Main class for portfolio optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model: nn.Module,\n",
    "                 learning_rate: float = 1e-4,\n",
    "                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the optimizer\n",
    "        \n",
    "        Args:\n",
    "            model: The portfolio optimization model\n",
    "            learning_rate: Learning rate for training\n",
    "            device: Device to use for training ('cuda' or 'cpu')\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def train(self, \n",
    "              train_loader: DataLoader, \n",
    "              val_loader: Optional[DataLoader] = None,\n",
    "              num_epochs: int = 50,\n",
    "              early_stopping_patience: int = 10):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \n",
    "        Args:\n",
    "            train_loader: DataLoader for training data\n",
    "            val_loader: DataLoader for validation data\n",
    "            num_epochs: Number of epochs to train for\n",
    "            early_stopping_patience: Number of epochs to wait for validation loss improvement\n",
    "        \"\"\"\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                # Forward pass\n",
    "                predicted_weights = self.model(\n",
    "                    batch['returns'],\n",
    "                    batch['features'],\n",
    "                    batch['mask'],\n",
    "                    batch['strategy'],\n",
    "                    batch['risk_tolerance']\n",
    "                )\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = self._calculate_loss(\n",
    "                    predicted_weights,\n",
    "                    batch['target_weights'],\n",
    "                    batch['mask'],\n",
    "                    batch['strategy']\n",
    "                )\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            if val_loader is not None:\n",
    "                val_loss = self.evaluate(val_loader)\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "                \n",
    "                # Early stopping\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    # Save best model\n",
    "                    torch.save(self.model.state_dict(), 'best_portfolio_model.pth')\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= early_stopping_patience:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.6f}\")\n",
    "        \n",
    "        # Load best model if validation was used\n",
    "        if val_loader is not None:\n",
    "            self.model.load_state_dict(torch.load('best_portfolio_model.pth'))\n",
    "            \n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def evaluate(self, data_loader: DataLoader) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the model\n",
    "        \n",
    "        Args:\n",
    "            data_loader: DataLoader for evaluation data\n",
    "            \n",
    "        Returns:\n",
    "            Average loss on the evaluation data\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                # Forward pass\n",
    "                predicted_weights = self.model(\n",
    "                    batch['returns'],\n",
    "                    batch['features'],\n",
    "                    batch['mask'],\n",
    "                    batch['strategy'],\n",
    "                    batch['risk_tolerance']\n",
    "                )\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = self._calculate_loss(\n",
    "                    predicted_weights,\n",
    "                    batch['target_weights'],\n",
    "                    batch['mask'],\n",
    "                    batch['strategy']\n",
    "                )\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(data_loader)\n",
    "    \n",
    "    def predict(self, \n",
    "                returns: np.ndarray,\n",
    "                features: np.ndarray,\n",
    "                active_tickers: List[str],\n",
    "                strategy_idx: int = 0,\n",
    "                risk_tolerance: int = 5) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Generate portfolio weights for given input\n",
    "        \n",
    "        Args:\n",
    "            returns: Historical returns array [window_size, num_assets]\n",
    "            features: Features array [window_size, num_assets, feature_dim]\n",
    "            active_tickers: List of tickers to include in the portfolio\n",
    "            strategy_idx: Strategy index (0: Max Sharpe, 1: Min Vol, 2: ERC)\n",
    "            risk_tolerance: Risk tolerance level (1-10)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping tickers to weights\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Create mask based on active tickers\n",
    "        all_tickers = TOP_STOCKS\n",
    "        mask = np.zeros(len(all_tickers))\n",
    "        for ticker in active_tickers:\n",
    "            if ticker in all_tickers:\n",
    "                idx = all_tickers.index(ticker)\n",
    "                mask[idx] = 1\n",
    "        \n",
    "        # Prepare strategy one-hot encoding\n",
    "        strategy_onehot = np.zeros(3)\n",
    "        strategy_onehot[strategy_idx] = 1\n",
    "        \n",
    "        # Convert inputs to tensors and add batch dimension\n",
    "        returns_tensor = torch.tensor(returns, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        mask_tensor = torch.tensor(mask, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        strategy_tensor = torch.tensor(strategy_onehot, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        risk_tensor = torch.tensor([risk_tolerance/10.0], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Generate weights\n",
    "        with torch.no_grad():\n",
    "            predicted_weights = self.model(\n",
    "                returns_tensor,\n",
    "                features_tensor,\n",
    "                mask_tensor,\n",
    "                strategy_tensor,\n",
    "                risk_tensor\n",
    "            )\n",
    "        \n",
    "        # Convert to numpy and remove batch dimension\n",
    "        weights = predicted_weights.cpu().numpy()[0]\n",
    "        \n",
    "        # Create dictionary mapping tickers to weights\n",
    "        portfolio = {}\n",
    "        for i, ticker in enumerate(all_tickers):\n",
    "            if mask[i] > 0 and weights[i] > 0.001:  # Only include assets with non-trivial weights\n",
    "                portfolio[ticker] = weights[i]\n",
    "        \n",
    "        return portfolio\n",
    "    \n",
    "    def _calculate_loss(self, \n",
    "                        predicted_weights: torch.Tensor,\n",
    "                        target_weights: torch.Tensor,\n",
    "                        mask: torch.Tensor,\n",
    "                        strategy: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the loss based on the optimization strategy\n",
    "        \n",
    "        Args:\n",
    "            predicted_weights: Predicted portfolio weights\n",
    "            target_weights: Target portfolio weights\n",
    "            mask: Mask indicating active assets\n",
    "            strategy: One-hot encoded strategy\n",
    "            \n",
    "        Returns:\n",
    "            Loss value\n",
    "        \"\"\"\n",
    "        # Mean squared error between predicted and target weights\n",
    "        mse_loss = torch.mean(((predicted_weights - target_weights) ** 2) * mask)\n",
    "        \n",
    "        # Add regularization for weight concentration\n",
    "        sum_squared = torch.sum(predicted_weights ** 2, dim=1)\n",
    "        concentration_penalty = torch.mean(sum_squared)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = mse_loss + 0.01 * concentration_penalty\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "def prepare_data_pipeline(fetch_historical_data_fn, tickers=TOP_STOCKS, window_size=252):\n",
    "    \"\"\"\n",
    "    Prepare data pipeline for model training and evaluation\n",
    "    \n",
    "    Args:\n",
    "        fetch_historical_data_fn: Function to fetch historical data for a ticker\n",
    "        tickers: List of tickers to include\n",
    "        window_size: Number of days to use for historical window\n",
    "        \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader\n",
    "    \"\"\"\n",
    "    # Fetch data for all tickers\n",
    "    raw_data = {}\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            raw_data[ticker] = fetch_historical_data_fn(ticker)\n",
    "            print(f\"Fetched data for {ticker}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {ticker}: {e}\")\n",
    "    \n",
    "    # Process data\n",
    "    processor = DataProcessor()\n",
    "    \n",
    "    # Calculate returns and features\n",
    "    returns_dict = {}\n",
    "    features_dict = {}\n",
    "    \n",
    "    for ticker, df in raw_data.items():\n",
    "        # Calculate returns\n",
    "        returns = processor.calculate_returns(df)\n",
    "        returns_dict[ticker] = returns\n",
    "        \n",
    "        # Calculate features\n",
    "        features = processor.calculate_features(df)\n",
    "        features_dict[ticker] = features\n",
    "    \n",
    "    # Align data across all assets\n",
    "    returns_dict = processor.align_data_for_multiple_assets(returns_dict)\n",
    "    features_dict = processor.align_data_for_multiple_assets(features_dict)\n",
    "    \n",
    "    # Normalize features\n",
    "    features_dict = processor.normalize_features(features_dict)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PortfolioDataset(\n",
    "        returns_dict=returns_dict,\n",
    "        features_dict=features_dict,\n",
    "        window_size=window_size,\n",
    "        num_samples=5000,\n",
    "        tickers=tickers,\n",
    "        training=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = PortfolioDataset(\n",
    "        returns_dict=returns_dict,\n",
    "        features_dict=features_dict,\n",
    "        window_size=window_size,\n",
    "        num_samples=1000,\n",
    "        tickers=tickers,\n",
    "        training=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = PortfolioDataset(\n",
    "        returns_dict=returns_dict,\n",
    "        features_dict=features_dict,\n",
    "        window_size=window_size,\n",
    "        num_samples=1000,\n",
    "        tickers=tickers,\n",
    "        training=False\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, (returns_dict, features_dict)\n",
    "\n",
    "\n",
    "def train_model(train_loader, val_loader, num_assets=10, window_size=252):\n",
    "    \"\"\"\n",
    "    Train the portfolio optimization model\n",
    "    \n",
    "    Args:\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        num_assets: Number of assets in the universe\n",
    "        window_size: Number of days in the historical window\n",
    "        \n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    model = PortfolioOptimizationModel(\n",
    "        num_assets=num_assets,\n",
    "        window_size=window_size,\n",
    "        hidden_dim=64,\n",
    "        feature_dim=4\n",
    "    )\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = PortfolioOptimizer(model, learning_rate=1e-4)\n",
    "    \n",
    "    # Train model\n",
    "    train_losses, val_losses = optimizer.train(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=1,\n",
    "        early_stopping_patience=10\n",
    "    )\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def evaluate_model(model_optimizer, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data\n",
    "    \n",
    "    Args:\n",
    "        model_optimizer: Trained model optimizer\n",
    "        test_loader: DataLoader for test data\n",
    "        \n",
    "    Returns:\n",
    "        Test loss\n",
    "    \"\"\"\n",
    "    test_loss = model_optimizer.evaluate(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.6f}\")\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def backtest_portfolio(model_optimizer, data_tuple, tickers=TOP_STOCKS, window_size=252, \n",
    "                       test_period=60, strategy_idx=0, risk_tolerance=5):\n",
    "    \"\"\"\n",
    "    Backtest the portfolio optimization model\n",
    "    \n",
    "    Args:\n",
    "        model_optimizer: Trained model optimizer\n",
    "        data_tuple: Tuple of (returns_dict, features_dict)\n",
    "        tickers: List of tickers to include\n",
    "        window_size: Number of days in the historical window\n",
    "        test_period: Number of days to test\n",
    "        strategy_idx: Strategy index (0: Max Sharpe, 1: Min Vol, 2: ERC)\n",
    "        risk_tolerance: Risk tolerance level (1-10)\n",
    "        \n",
    "    Returns:\n",
    "        Backtest results\n",
    "    \"\"\"\n",
    "    returns_dict, features_dict = data_tuple\n",
    "    \n",
    "    # Get common dates\n",
    "    common_dates = sorted(list(returns_dict[tickers[0]].index))\n",
    "    \n",
    "    # Select test period\n",
    "    test_dates = common_dates[-test_period:]\n",
    "    \n",
    "    # Initialize results\n",
    "    portfolio_values = [1.0]\n",
    "    portfolio_weights_history = []\n",
    "    \n",
    "    # Run backtest\n",
    "    for i in range(len(test_dates) - 1):\n",
    "        current_date = test_dates[i]\n",
    "        next_date = test_dates[i+1]\n",
    "        \n",
    "        # Get historical window ending on current date\n",
    "        window_end_idx = common_dates.index(current_date)\n",
    "        window_start_idx = window_end_idx - window_size + 1\n",
    "        window_dates = common_dates[window_start_idx:window_end_idx+1]\n",
    "        \n",
    "        # Prepare input data\n",
    "        returns_window = np.zeros((window_size, len(tickers)))\n",
    "        features_window = np.zeros((window_size, len(tickers), 4))\n",
    "        \n",
    "        for j, ticker in enumerate(tickers):\n",
    "            returns_window[:, j] = returns_dict[ticker].loc[window_dates].values\n",
    "            features_window[:, j, :] = features_dict[ticker].loc[window_dates].values\n",
    "        \n",
    "        # Generate portfolio weights\n",
    "        portfolio_weights = model_optimizer.predict(\n",
    "            returns=returns_window,\n",
    "            features=features_window,\n",
    "            active_tickers=tickers,\n",
    "            strategy_idx=strategy_idx,\n",
    "            risk_tolerance=risk_tolerance\n",
    "        )\n",
    "        \n",
    "        portfolio_weights_history.append(portfolio_weights)\n",
    "        \n",
    "        # Calculate portfolio return\n",
    "        portfolio_return = 0.0\n",
    "        for ticker, weight in portfolio_weights.items():\n",
    "            ticker_idx = tickers.index(ticker)\n",
    "            ticker_return = returns_dict[ticker].loc[next_date]\n",
    "            portfolio_return += weight * ticker_return\n",
    "        \n",
    "        # Update portfolio value\n",
    "        portfolio_values.append(portfolio_values[-1] * (1 + portfolio_return))\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    portfolio_returns = np.diff(portfolio_values) / portfolio_values[:-1]\n",
    "    annual_return = np.mean(portfolio_returns) * 252\n",
    "    annual_volatility = np.std(portfolio_returns) * np.sqrt(252)\n",
    "    sharpe_ratio = annual_return / annual_volatility if annual_volatility > 0 else 0\n",
    "    max_drawdown = np.max(np.maximum.accumulate(portfolio_values) - portfolio_values) / np.max(portfolio_values)\n",
    "    \n",
    "    # Plot portfolio performance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(test_dates, portfolio_values)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Portfolio Value')\n",
    "    plt.title('Portfolio Backtest Performance')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot portfolio composition\n",
    "    plt.subplot(2, 1, 2)\n",
    "    weights_data = []\n",
    "    for weights in portfolio_weights_history:\n",
    "        weight_vector = []\n",
    "        for ticker in tickers:\n",
    "            weight_vector.append(weights.get(ticker, 0))\n",
    "        weights_data.append(weight_vector)\n",
    "    \n",
    "    weights_array = np.array(weights_data)\n",
    "    plt.stackplot(test_dates[:-1], weights_array.T, labels=tickers, alpha=0.7)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Weight')\n",
    "    plt.title('Portfolio Composition')\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('backtest_results.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return results\n",
    "    results = {\n",
    "        'portfolio_values': portfolio_values,\n",
    "        'annual_return': annual_return,\n",
    "        'annual_volatility': annual_volatility,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'portfolio_weights_history': portfolio_weights_history\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_strategies(model_optimizer, data_tuple, tickers=TOP_STOCKS, window_size=252, test_period=60):\n",
    "    \"\"\"\n",
    "    Compare different portfolio optimization strategies\n",
    "    \n",
    "    Args:\n",
    "        model_optimizer: Trained model optimizer\n",
    "        data_tuple: Tuple of (returns_dict, features_dict)\n",
    "        tickers: List of tickers to include\n",
    "        window_size: Number of days in the historical window\n",
    "        test_period: Number of days to test\n",
    "        \n",
    "    Returns:\n",
    "        Comparison results\n",
    "    \"\"\"\n",
    "    strategies = [\n",
    "        {'name': 'Maximum Sharpe Ratio', 'idx': 0, 'risk': 5},\n",
    "        {'name': 'Minimum Volatility', 'idx': 1, 'risk': 5},\n",
    "        {'name': 'Equal Risk Contribution', 'idx': 2, 'risk': 5},\n",
    "        {'name': 'High Risk (Max Sharpe)', 'idx': 0, 'risk': 8},\n",
    "        {'name': 'Low Risk (Min Vol)', 'idx': 1, 'risk': 2}\n",
    "    ]\n",
    "    \n",
    "    # Run backtest for each strategy\n",
    "    results = {}\n",
    "    for strategy in strategies:\n",
    "        print(f\"Backtesting {strategy['name']}...\")\n",
    "        strategy_results = backtest_portfolio(\n",
    "            model_optimizer=model_optimizer,\n",
    "            data_tuple=data_tuple,\n",
    "            tickers=tickers,\n",
    "            window_size=window_size,\n",
    "            test_period=test_period,\n",
    "            strategy_idx=strategy['idx'],\n",
    "            risk_tolerance=strategy['risk']\n",
    "        )\n",
    "        results[strategy['name']] = strategy_results\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for strategy_name, strategy_results in results.items():\n",
    "        plt.plot(strategy_results['portfolio_values'], label=strategy_name)\n",
    "    \n",
    "    plt.xlabel('Trading Day')\n",
    "    plt.ylabel('Portfolio Value')\n",
    "    plt.title('Strategy Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('strategy_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Compile performance metrics\n",
    "    performance_summary = pd.DataFrame(columns=['Annual Return', 'Annual Volatility', 'Sharpe Ratio', 'Max Drawdown'])\n",
    "    \n",
    "    for strategy_name, strategy_results in results.items():\n",
    "        performance_summary.loc[strategy_name] = [\n",
    "            f\"{strategy_results['annual_return']*100:.2f}%\",\n",
    "            f\"{strategy_results['annual_volatility']*100:.2f}%\",\n",
    "            f\"{strategy_results['sharpe_ratio']:.2f}\",\n",
    "            f\"{strategy_results['max_drawdown']*100:.2f}%\"\n",
    "        ]\n",
    "    \n",
    "    return performance_summary, results\n",
    "\n",
    "\n",
    "def optimize_portfolio_for_subset(model_optimizer, data_tuple, subset_tickers, \n",
    "                                  strategy_name=\"Maximum Sharpe Ratio\", risk_tolerance=5):\n",
    "    \"\"\"\n",
    "    Optimize a portfolio for a subset of assets\n",
    "    \n",
    "    Args:\n",
    "        model_optimizer: Trained model optimizer\n",
    "        data_tuple: Tuple of (returns_dict, features_dict)\n",
    "        subset_tickers: List of tickers in the subset\n",
    "        strategy_name: Strategy name\n",
    "        risk_tolerance: Risk tolerance level (1-10)\n",
    "        \n",
    "    Returns:\n",
    "        Optimized portfolio weights\n",
    "    \"\"\"\n",
    "    returns_dict, features_dict = data_tuple\n",
    "    all_tickers = TOP_STOCKS\n",
    "    \n",
    "    # Map strategy name to index\n",
    "    strategy_map = {\n",
    "        \"Maximum Sharpe Ratio\": 0,\n",
    "        \"Minimum Volatility\": 1,\n",
    "        \"Equal Risk Contribution\": 2\n",
    "    }\n",
    "    strategy_idx = strategy_map.get(strategy_name, 0)\n",
    "    \n",
    "    # Get common dates\n",
    "    common_dates = sorted(list(returns_dict[all_tickers[0]].index))\n",
    "    window_size = 252\n",
    "    \n",
    "    # Use the most recent window\n",
    "    window_dates = common_dates[-window_size:]\n",
    "    \n",
    "    # Prepare input data\n",
    "    returns_window = np.zeros((window_size, len(all_tickers)))\n",
    "    features_window = np.zeros((window_size, len(all_tickers), 4))\n",
    "    \n",
    "    for j, ticker in enumerate(all_tickers):\n",
    "        returns_window[:, j] = returns_dict[ticker].loc[window_dates].values\n",
    "        features_window[:, j, :] = features_dict[ticker].loc[window_dates].values\n",
    "    \n",
    "    # Generate portfolio weights\n",
    "    portfolio_weights = model_optimizer.predict(\n",
    "        returns=returns_window,\n",
    "        features=features_window,\n",
    "        active_tickers=subset_tickers,\n",
    "        strategy_idx=strategy_idx,\n",
    "        risk_tolerance=risk_tolerance\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    result_df = pd.DataFrame(columns=['Weight'])\n",
    "    for ticker, weight in portfolio_weights.items():\n",
    "        result_df.loc[ticker] = [f\"{weight*100:.2f}%\"]\n",
    "    \n",
    "    # Calculate expected portfolio metrics\n",
    "    subset_returns = np.zeros((len(window_dates), len(subset_tickers)))\n",
    "    for i, ticker in enumerate(subset_tickers):\n",
    "        ticker_idx = all_tickers.index(ticker)\n",
    "        subset_returns[:, i] = returns_window[:, ticker_idx]\n",
    "    \n",
    "    weights_array = np.array([portfolio_weights.get(ticker, 0) for ticker in subset_tickers])\n",
    "    \n",
    "    # Calculate portfolio metrics\n",
    "    mean_returns = np.mean(subset_returns, axis=0) * 252\n",
    "    cov_matrix = np.cov(subset_returns.T) * 252\n",
    "    \n",
    "    portfolio_return = np.sum(mean_returns * weights_array)\n",
    "    portfolio_volatility = np.sqrt(weights_array.T @ cov_matrix @ weights_array)\n",
    "    sharpe_ratio = portfolio_return / portfolio_volatility if portfolio_volatility > 0 else 0\n",
    "    \n",
    "    # Format portfolio metrics\n",
    "    metrics = {\n",
    "        \"Expected Annual Return\": f\"{portfolio_return*100:.2f}%\",\n",
    "        \"Expected Annual Volatility\": f\"{portfolio_volatility*100:.2f}%\",\n",
    "        \"Expected Sharpe Ratio\": f\"{sharpe_ratio:.2f}\"\n",
    "    }\n",
    "    \n",
    "    return result_df, metrics\n",
    "\n",
    "\n",
    "def main(fetch_historical_data_fn):\n",
    "    \"\"\"\n",
    "    Main function to run the portfolio optimization pipeline\n",
    "    \n",
    "    Args:\n",
    "        fetch_historical_data_fn: Function to fetch historical data for a ticker\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    print(\"Preparing data...\")\n",
    "    train_loader, val_loader, test_loader, data_tuple = prepare_data_pipeline(\n",
    "        fetch_historical_data_fn=fetch_historical_data_fn,\n",
    "        tickers=TOP_STOCKS,\n",
    "        window_size=252\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    model, optimizer = train_model(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_assets=len(TOP_STOCKS),\n",
    "        window_size=252\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "    test_loss = evaluate_model(\n",
    "        model_optimizer=optimizer,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "    \n",
    "    # Backtest model\n",
    "    print(\"Running backtest...\")\n",
    "    backtest_results = backtest_portfolio(\n",
    "        model_optimizer=optimizer,\n",
    "        data_tuple=data_tuple,\n",
    "        tickers=TOP_STOCKS,\n",
    "        window_size=252,\n",
    "        test_period=60\n",
    "    )\n",
    "    \n",
    "    # Compare strategies\n",
    "    print(\"Comparing strategies...\")\n",
    "    performance_summary, strategy_results = compare_strategies(\n",
    "        model_optimizer=optimizer,\n",
    "        data_tuple=data_tuple,\n",
    "        tickers=TOP_STOCKS,\n",
    "        window_size=252,\n",
    "        test_period=60\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(performance_summary)\n",
    "    \n",
    "    # Example: Optimize a 5-asset portfolio\n",
    "    subset_tickers = ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META']\n",
    "    print(f\"\\nOptimizing portfolio for subset: {subset_tickers}\")\n",
    "    \n",
    "    weights_df, metrics = optimize_portfolio_for_subset(\n",
    "        model_optimizer=optimizer,\n",
    "        data_tuple=data_tuple,\n",
    "        subset_tickers=subset_tickers,\n",
    "        strategy_name=\"Maximum Sharpe Ratio\",\n",
    "        risk_tolerance=5\n",
    "    )\n",
    "    \n",
    "    print(\"\\nOptimized Weights:\")\n",
    "    print(weights_df)\n",
    "    \n",
    "    print(\"\\nExpected Portfolio Metrics:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "    \n",
    "    print(\"\\nAll done!\")\n",
    "\n",
    "def fetch_historical_data(ticker: str):\n",
    "\n",
    "        end_date = datetime.today()\n",
    "        start_date = end_date - timedelta(days=1825)\n",
    "        end_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "        start_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        stock = yf.Ticker(ticker)\n",
    "        hist = stock.history(start=start_str, end=end_str)\n",
    "        hist.reset_index(inplace=True)\n",
    "\n",
    "        return hist\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # You would replace this with your actual function to fetch historical data\n",
    "\n",
    "    main(fetch_historical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e06325a6-b008-4058-aadc-3ba7fd90c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, UploadFile, File, Form, BackgroundTasks\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional, Union, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# # Import the existing classes (assumed to be in a separate file)\n",
    "# from portfolio_model import (\n",
    "#     DataProcessor, PortfolioDataset, PortfolioOptimizationModel, \n",
    "#     PortfolioOptimizer, TOP_STOCKS\n",
    "# )\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Portfolio Optimization API\",\n",
    "    description=\"API for ML-based portfolio optimization with universal and local modes\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Define model storage paths\n",
    "MODEL_DIR = \"models\"\n",
    "DATA_DIR = \"data\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Global model and data storage\n",
    "model_storage = {}\n",
    "data_storage = {}\n",
    "training_status = {}\n",
    "\n",
    "# Pydantic models for request/response validation\n",
    "class AssetData(BaseModel):\n",
    "    ticker: str\n",
    "    data: Dict[str, List[float]]  # Date -> [Open, High, Low, Close, Volume]\n",
    "\n",
    "class TrainingRequest(BaseModel):\n",
    "    model_id: str\n",
    "    num_epochs: int = 50\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 1e-4\n",
    "    hidden_dim: int = 64\n",
    "\n",
    "class OptimizationRequest(BaseModel):\n",
    "    model_id: str\n",
    "    subset_tickers: Optional[List[str]] = None\n",
    "    strategy: str = \"Maximum Sharpe Ratio\"\n",
    "    risk_tolerance: int = Field(5, ge=1, le=10)\n",
    "\n",
    "class LocalModeRequest(BaseModel):\n",
    "    model_id: str\n",
    "    asset_mask: List[int]  # Binary vector\n",
    "    strategy: int = Field(0, ge=0, le=2)\n",
    "    risk_tolerance: int = Field(5, ge=1, le=10)\n",
    "\n",
    "class ModelInfoResponse(BaseModel):\n",
    "    model_id: str\n",
    "    tickers: List[str]\n",
    "    trained: bool\n",
    "    last_updated: str\n",
    "    window_size: int\n",
    "\n",
    "class PortfolioOptimizationAPI:\n",
    "    \"\"\"API for portfolio optimization model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_id: str, window_size: int = 252):\n",
    "        \"\"\"Initialize API with a model ID\"\"\"\n",
    "        self.model_id = model_id\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.data_tuple = None\n",
    "        self.tickers = TOP_STOCKS.copy()\n",
    "        self.trained = False\n",
    "        self.window_size = window_size\n",
    "        self.last_updated = datetime.now().isoformat()\n",
    "    \n",
    "    def universal_mode(self, asset_data: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"Set up universal mode with user-provided assets\"\"\"\n",
    "        # Validate input data format\n",
    "        for ticker, df in asset_data.items():\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                raise ValueError(f\"Data for {ticker} must be a pandas DataFrame\")\n",
    "            if 'Close' not in df.columns:\n",
    "                raise ValueError(f\"DataFrame for {ticker} must contain a 'Close' column\")\n",
    "        \n",
    "        # Update tickers list\n",
    "        self.tickers = list(asset_data.keys())\n",
    "        logger.info(f\"Universal mode activated with {len(self.tickers)} assets: {self.tickers}\")\n",
    "        \n",
    "        # Process data\n",
    "        processor = DataProcessor()\n",
    "        \n",
    "        # Calculate returns and features\n",
    "        returns_dict = {}\n",
    "        features_dict = {}\n",
    "        \n",
    "        for ticker, df in asset_data.items():\n",
    "            # Calculate returns\n",
    "            returns = processor.calculate_returns(df)\n",
    "            returns_dict[ticker] = returns\n",
    "            \n",
    "            # Calculate features\n",
    "            features = processor.calculate_features(df)\n",
    "            features_dict[ticker] = features\n",
    "        \n",
    "        # Align data across all assets\n",
    "        returns_dict = processor.align_data_for_multiple_assets(returns_dict)\n",
    "        features_dict = processor.align_data_for_multiple_assets(features_dict)\n",
    "        \n",
    "        # Normalize features\n",
    "        features_dict = processor.normalize_features(features_dict)\n",
    "        \n",
    "        # Store data\n",
    "        self.data_tuple = (returns_dict, features_dict)\n",
    "        self.last_updated = datetime.now().isoformat()\n",
    "        \n",
    "        # Return status\n",
    "        return {\n",
    "            \"status\": \"success\", \n",
    "            \"message\": f\"Universal mode set up with {len(self.tickers)} assets\"\n",
    "        }\n",
    "    \n",
    "    def train_model(self, \n",
    "                   num_epochs: int = 50, \n",
    "                   batch_size: int = 32, \n",
    "                   learning_rate: float = 1e-4, \n",
    "                   hidden_dim: int = 64):\n",
    "        \"\"\"Train the model with current data\"\"\"\n",
    "        if self.data_tuple is None:\n",
    "            raise ValueError(\"No data available. Call universal_mode first.\")\n",
    "        \n",
    "        returns_dict, features_dict = self.data_tuple\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = PortfolioDataset(\n",
    "            returns_dict=returns_dict,\n",
    "            features_dict=features_dict,\n",
    "            window_size=self.window_size,\n",
    "            num_samples=5000,\n",
    "            tickers=self.tickers,\n",
    "            training=True\n",
    "        )\n",
    "        \n",
    "        val_dataset = PortfolioDataset(\n",
    "            returns_dict=returns_dict,\n",
    "            features_dict=features_dict,\n",
    "            window_size=self.window_size,\n",
    "            num_samples=1000,\n",
    "            tickers=self.tickers,\n",
    "            training=True\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = PortfolioOptimizationModel(\n",
    "            num_assets=len(self.tickers),\n",
    "            window_size=self.window_size,\n",
    "            hidden_dim=hidden_dim,\n",
    "            feature_dim=4\n",
    "        )\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        self.optimizer = PortfolioOptimizer(\n",
    "            model=self.model, \n",
    "            learning_rate=learning_rate,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        logger.info(f\"Training model with {len(self.tickers)} assets for {num_epochs} epochs...\")\n",
    "        training_status[self.model_id] = \"training\"\n",
    "        \n",
    "        try:\n",
    "            train_losses, val_losses = self.optimizer.train(\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                num_epochs=num_epochs,\n",
    "                early_stopping_patience=10\n",
    "            )\n",
    "            \n",
    "            self.trained = True\n",
    "            self.last_updated = datetime.now().isoformat()\n",
    "            training_status[self.model_id] = \"completed\"\n",
    "            \n",
    "            # Save model and data\n",
    "            self._save_model()\n",
    "            self._save_data()\n",
    "            \n",
    "            # Return training results\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"message\": \"Model trained successfully\",\n",
    "                \"metrics\": {\n",
    "                    \"final_train_loss\": float(train_losses[-1]),\n",
    "                    \"final_val_loss\": float(val_losses[-1]),\n",
    "                    \"epochs_completed\": len(train_losses)\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            training_status[self.model_id] = f\"failed: {str(e)}\"\n",
    "            logger.error(f\"Training failed: {str(e)}\")\n",
    "            raise e\n",
    "    \n",
    "    def optimize_portfolio(self, \n",
    "                          subset_tickers: List[str] = None, \n",
    "                          strategy: str = \"Maximum Sharpe Ratio\", \n",
    "                          risk_tolerance: int = 5):\n",
    "        \"\"\"Generate optimal portfolio weights for a subset of assets (Universal Mode)\"\"\"\n",
    "        if not self.trained:\n",
    "            raise ValueError(\"Model not trained. Call train_model first.\")\n",
    "        \n",
    "        if subset_tickers is None:\n",
    "            subset_tickers = self.tickers\n",
    "        else:\n",
    "            # Validate that all tickers exist in our universe\n",
    "            for ticker in subset_tickers:\n",
    "                if ticker not in self.tickers:\n",
    "                    raise ValueError(f\"Ticker {ticker} not found in model universe\")\n",
    "        \n",
    "        # Map strategy name to index\n",
    "        strategy_map = {\n",
    "            \"Maximum Sharpe Ratio\": 0,\n",
    "            \"Minimum Volatility\": 1,\n",
    "            \"Equal Risk Contribution\": 2\n",
    "        }\n",
    "        strategy_idx = strategy_map.get(strategy, 0)\n",
    "        \n",
    "        returns_dict, features_dict = self.data_tuple\n",
    "        \n",
    "        # Get common dates\n",
    "        common_dates = sorted(list(returns_dict[self.tickers[0]].index))\n",
    "        \n",
    "        # Use the most recent window\n",
    "        window_dates = common_dates[-self.window_size:]\n",
    "        \n",
    "        # Prepare input data\n",
    "        returns_window = np.zeros((self.window_size, len(self.tickers)))\n",
    "        features_window = np.zeros((self.window_size, len(self.tickers), 4))\n",
    "        \n",
    "        for j, ticker in enumerate(self.tickers):\n",
    "            returns_window[:, j] = returns_dict[ticker].loc[window_dates].values\n",
    "            features_window[:, j, :] = features_dict[ticker].loc[window_dates].values\n",
    "        \n",
    "        # Generate portfolio weights\n",
    "        portfolio_weights = self.optimizer.predict(\n",
    "            returns=returns_window,\n",
    "            features=features_window,\n",
    "            active_tickers=subset_tickers,\n",
    "            strategy_idx=strategy_idx,\n",
    "            risk_tolerance=risk_tolerance\n",
    "        )\n",
    "        \n",
    "        # Calculate expected portfolio metrics\n",
    "        active_indices = [self.tickers.index(ticker) for ticker in subset_tickers if ticker in self.tickers]\n",
    "        subset_returns = returns_window[:, active_indices]\n",
    "        \n",
    "        weights_array = np.array([portfolio_weights.get(ticker, 0) for ticker in subset_tickers])\n",
    "        \n",
    "        # Calculate portfolio metrics\n",
    "        mean_returns = np.mean(subset_returns, axis=0) * 252\n",
    "        cov_matrix = np.cov(subset_returns.T) * 252\n",
    "        \n",
    "        portfolio_return = np.sum(mean_returns * weights_array)\n",
    "        portfolio_volatility = np.sqrt(weights_array.T @ cov_matrix @ weights_array)\n",
    "        sharpe_ratio = portfolio_return / portfolio_volatility if portfolio_volatility > 0 else 0\n",
    "        \n",
    "        # Format portfolio metrics\n",
    "        metrics = {\n",
    "            \"Expected Annual Return\": f\"{portfolio_return*100:.2f}%\",\n",
    "            \"Expected Annual Volatility\": f\"{portfolio_volatility*100:.2f}%\",\n",
    "            \"Expected Sharpe Ratio\": f\"{sharpe_ratio:.2f}\"\n",
    "        }\n",
    "        \n",
    "        # Format weights for response\n",
    "        formatted_weights = {ticker: float(weight) for ticker, weight in portfolio_weights.items()}\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"portfolio_weights\": formatted_weights,\n",
    "            \"metrics\": metrics\n",
    "        }\n",
    "    \n",
    "    def local_mode_predict(self, asset_mask: List[int], strategy_idx: int = 0, risk_tolerance: int = 5):\n",
    "        \"\"\"Generate optimal portfolio weights using a binary mask (Local Mode)\"\"\"\n",
    "        if not self.trained:\n",
    "            raise ValueError(\"Model not trained. Call train_model first.\")\n",
    "        \n",
    "        # Validate mask length\n",
    "        if len(asset_mask) != len(self.tickers):\n",
    "            raise ValueError(f\"Asset mask length ({len(asset_mask)}) must match number of assets ({len(self.tickers)})\")\n",
    "        \n",
    "        # Convert mask to numpy array\n",
    "        mask = np.array(asset_mask, dtype=np.float32)\n",
    "        \n",
    "        # Get active tickers\n",
    "        active_tickers = [ticker for i, ticker in enumerate(self.tickers) if mask[i] == 1]\n",
    "        \n",
    "        if len(active_tickers) == 0:\n",
    "            raise ValueError(\"At least one asset must be active in the mask\")\n",
    "        \n",
    "        returns_dict, features_dict = self.data_tuple\n",
    "        \n",
    "        # Get common dates\n",
    "        common_dates = sorted(list(returns_dict[self.tickers[0]].index))\n",
    "        \n",
    "        # Use the most recent window\n",
    "        window_dates = common_dates[-self.window_size:]\n",
    "        \n",
    "        # Prepare input data\n",
    "        returns_window = np.zeros((self.window_size, len(self.tickers)))\n",
    "        features_window = np.zeros((self.window_size, len(self.tickers), 4))\n",
    "        \n",
    "        for j, ticker in enumerate(self.tickers):\n",
    "            returns_window[:, j] = returns_dict[ticker].loc[window_dates].values\n",
    "            features_window[:, j, :] = features_dict[ticker].loc[window_dates].values\n",
    "        \n",
    "        # Generate portfolio weights\n",
    "        portfolio_weights = self.optimizer.predict(\n",
    "            returns=returns_window,\n",
    "            features=features_window,\n",
    "            active_tickers=active_tickers,\n",
    "            strategy_idx=strategy_idx,\n",
    "            risk_tolerance=risk_tolerance\n",
    "        )\n",
    "        \n",
    "        # Create weight vector in the same order as the mask\n",
    "        weight_vector = np.zeros(len(self.tickers))\n",
    "        for ticker, weight in portfolio_weights.items():\n",
    "            idx = self.tickers.index(ticker)\n",
    "            weight_vector[idx] = weight\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"weight_vector\": weight_vector.tolist(),\n",
    "            \"portfolio_weights\": {ticker: float(weight) for ticker, weight in portfolio_weights.items()}\n",
    "        }\n",
    "    \n",
    "    def _save_model(self):\n",
    "        \"\"\"Save model to disk\"\"\"\n",
    "        model_path = os.path.join(MODEL_DIR, f\"{self.model_id}_model.pth\")\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "        \n",
    "    def _save_data(self):\n",
    "        \"\"\"Save data to disk\"\"\"\n",
    "        data_path = os.path.join(DATA_DIR, f\"{self.model_id}_data.pkl\")\n",
    "        with open(data_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'tickers': self.tickers,\n",
    "                'data_tuple': self.data_tuple,\n",
    "                'window_size': self.window_size,\n",
    "                'trained': self.trained,\n",
    "                'last_updated': self.last_updated\n",
    "            }, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, model_id: str):\n",
    "        \"\"\"Load model and data from disk\"\"\"\n",
    "        data_path = os.path.join(DATA_DIR, f\"{model_id}_data.pkl\")\n",
    "        model_path = os.path.join(MODEL_DIR, f\"{model_id}_model.pth\")\n",
    "        \n",
    "        if not os.path.exists(data_path) or not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model {model_id} not found\")\n",
    "        \n",
    "        # Load data\n",
    "        with open(data_path, 'rb') as f:\n",
    "            data_dict = pickle.load(f)\n",
    "        \n",
    "        # Create new instance\n",
    "        api = cls(model_id, data_dict['window_size'])\n",
    "        api.tickers = data_dict['tickers']\n",
    "        api.data_tuple = data_dict['data_tuple']\n",
    "        api.trained = data_dict['trained']\n",
    "        api.last_updated = data_dict['last_updated']\n",
    "        \n",
    "        # Load model if trained\n",
    "        if api.trained:\n",
    "            api.model = PortfolioOptimizationModel(\n",
    "                num_assets=len(api.tickers),\n",
    "                window_size=api.window_size,\n",
    "                hidden_dim=64,\n",
    "                feature_dim=4\n",
    "            )\n",
    "            \n",
    "            api.model.load_state_dict(torch.load(model_path, map_location=api.device))\n",
    "            \n",
    "            api.optimizer = PortfolioOptimizer(\n",
    "                model=api.model,\n",
    "                device=api.device\n",
    "            )\n",
    "        \n",
    "        return api\n",
    "\n",
    "\n",
    "def get_or_create_api(model_id: str, window_size: int = 252):\n",
    "    \"\"\"Get existing API or create a new one\"\"\"\n",
    "    if model_id in model_storage:\n",
    "        return model_storage[model_id]\n",
    "    \n",
    "    # Try to load from disk\n",
    "    try:\n",
    "        api = PortfolioOptimizationAPI.load(model_id)\n",
    "        model_storage[model_id] = api\n",
    "        return api\n",
    "    except FileNotFoundError:\n",
    "        # Create new\n",
    "        api = PortfolioOptimizationAPI(model_id, window_size)\n",
    "        model_storage[model_id] = api\n",
    "        return api\n",
    "\n",
    "\n",
    "def parse_csv_data(file_content: bytes, ticker: str):\n",
    "    \"\"\"Parse CSV data into DataFrame\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(io.StringIO(file_content.decode('utf-8')))\n",
    "        \n",
    "        # Try to infer date column\n",
    "        date_columns = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "        if date_columns:\n",
    "            date_col = date_columns[0]\n",
    "            df[date_col] = pd.to_datetime(df[date_col])\n",
    "            df.set_index(date_col, inplace=True)\n",
    "        else:\n",
    "            # If no obvious date column, assume first column is date\n",
    "            df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0])\n",
    "            df.set_index(df.columns[0], inplace=True)\n",
    "        \n",
    "        # Check for required columns\n",
    "        required_cols = ['Open', 'High', 'Low', 'Close']\n",
    "        if not all(col in df.columns for col in required_cols):\n",
    "            # Try to infer columns\n",
    "            if len(df.columns) >= 4:\n",
    "                df.columns = required_cols + ['Volume'] + [f'Extra{i}' for i in range(len(df.columns) - 5)]\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing CSV for {ticker}: {str(e)}\")\n",
    "        raise ValueError(f\"Invalid CSV format for {ticker}: {str(e)}\")\n",
    "\n",
    "\n",
    "async def train_model_task(\n",
    "    model_id: str,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    hidden_dim: int\n",
    "):\n",
    "    \"\"\"Background task for model training\"\"\"\n",
    "    try:\n",
    "        api = get_or_create_api(model_id)\n",
    "        result = api.train_model(\n",
    "            num_epochs=num_epochs,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            hidden_dim=hidden_dim\n",
    "        )\n",
    "        logger.info(f\"Training completed for model {model_id}: {result}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed for model {model_id}: {str(e)}\")\n",
    "        training_status[model_id] = f\"failed: {str(e)}\"\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"API root endpoint\"\"\"\n",
    "    return {\n",
    "        \"message\": \"Portfolio Optimization API\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"documentation\": \"/docs\"\n",
    "    }\n",
    "\n",
    "\n",
    "@app.post(\"/models/create\")\n",
    "async def create_model(model_id: str = Form(...), window_size: int = Form(252)):\n",
    "    \"\"\"Create a new model\"\"\"\n",
    "    try:\n",
    "        api = get_or_create_api(model_id, window_size)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": f\"Model {model_id} created\",\n",
    "            \"model_id\": model_id,\n",
    "            \"window_size\": window_size\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating model: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "@app.get(\"/models/list\")\n",
    "async def list_models():\n",
    "    \"\"\"List all available models\"\"\"\n",
    "    models = []\n",
    "    \n",
    "    # List models in memory\n",
    "    for model_id, api in model_storage.items():\n",
    "        models.append({\n",
    "            \"model_id\": model_id,\n",
    "            \"tickers\": api.tickers,\n",
    "            \"trained\": api.trained,\n",
    "            \"last_updated\": api.last_updated,\n",
    "            \"window_size\": api.window_size\n",
    "        })\n",
    "    \n",
    "    # List models on disk\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        if filename.endswith(\"_data.pkl\"):\n",
    "            model_id = filename.replace(\"_data.pkl\", \"\")\n",
    "            if model_id not in [m[\"model_id\"] for m in models]:\n",
    "                try:\n",
    "                    api = PortfolioOptimizationAPI.load(model_id)\n",
    "                    models.append({\n",
    "                        \"model_id\": model_id,\n",
    "                        \"tickers\": api.tickers,\n",
    "                        \"trained\": api.trained,\n",
    "                        \"last_updated\": api.last_updated,\n",
    "                        \"window_size\": api.window_size\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error loading model {model_id}: {str(e)}\")\n",
    "    \n",
    "    return {\"models\": models}\n",
    "\n",
    "\n",
    "@app.get(\"/models/{model_id}\")\n",
    "async def get_model_info(model_id: str):\n",
    "    \"\"\"Get information about a specific model\"\"\"\n",
    "    try:\n",
    "        api = get_or_create_api(model_id)\n",
    "        return {\n",
    "            \"model_id\": model_id,\n",
    "            \"tickers\": api.tickers,\n",
    "            \"trained\": api.trained,\n",
    "            \"last_updated\": api.last_updated,\n",
    "            \"window_size\": api.window_size,\n",
    "            \"training_status\": training_status.get(model_id, \"not started\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting model info: {str(e)}\")\n",
    "        raise HTTPException(status_code=404, detail=f\"Model {model_id} not found\")\n",
    "\n",
    "\n",
    "@app.post(\"/models/{model_id}/universal/upload\")\n",
    "async def upload_asset_data(\n",
    "    model_id: str, \n",
    "    files: List[UploadFile] = File(...),\n",
    "    tickers: List[str] = Form(...)\n",
    "):\n",
    "    \"\"\"Upload asset data files for universal mode\"\"\"\n",
    "    if len(files) != len(tickers):\n",
    "        raise HTTPException(status_code=400, detail=\"Number of files must match number of tickers\")\n",
    "    \n",
    "    try:\n",
    "        api = get_or_create_api(model_id)\n",
    "        \n",
    "        # Parse uploaded files\n",
    "        asset_data = {}\n",
    "        for ticker, file in zip(tickers, files):\n",
    "            content = await file.read()\n",
    "            df = parse_csv_data(content, ticker)\n",
    "            asset_data[ticker] = df\n",
    "        \n",
    "        # Set up universal mode\n",
    "        result = api.universal_mode(asset_data)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error uploading asset data: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "@app.post(\"/models/{model_id}/universal/json\")\n",
    "async def upload_asset_data_json(model_id: str, assets: List[AssetData]):\n",
    "    \"\"\"Upload asset data as JSON for universal mode\"\"\"\n",
    "    try:\n",
    "        api = get_or_create_api(model_id)\n",
    "        \n",
    "        # Parse JSON data\n",
    "        asset_data = {}\n",
    "        for asset in assets:\n",
    "            # Convert to DataFrame\n",
    "            data = pd.DataFrame(asset.data)\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "            data.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "            asset_data[asset.ticker] = data\n",
    "        \n",
    "        # Set up universal mode\n",
    "        result = api.universal_mode(asset_data)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error uploading asset data: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "@app.post(\"/models/{model_id}/train\")\n",
    "async def train_model_endpoint(\n",
    "    request: TrainingRequest,\n",
    "    background_tasks: BackgroundTasks\n",
    "):\n",
    "    \"\"\"Train the model (asynchronously)\"\"\"\n",
    "    try:\n",
    "        api = get_or_create_api(request.model_id)\n",
    "        \n",
    "        if api.data_tuple is None:\n",
    "            raise HTTPException(status_code=400, detail=\"No data available. Upload asset data first.\")\n",
    "        \n",
    "        # Start training in background\n",
    "        background_tasks.add_task(\n",
    "            train_model_task,\n",
    "            model_id=request.model_id,\n",
    "            num_epochs=request.num_epochs,\n",
    "            batch_size=request.batch_size,\n",
    "            learning_rate=request.learning_rate,\n",
    "            hidden_dim=request.hidden_dim\n",
    "        )\n",
    "        \n",
    "        training_status[request.model_id] = \"queued\"\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": f\"Training started for model {request.model_id}. Check status with GET /models/{request.model_id}\",\n",
    "            \"training_status\": \"queued\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error starting training: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "@app.get(\"/models/{model_id}/train/status\")\n",
    "async def get_training_status(model_id: str):\n",
    "    \"\"\"Get the training status of a model\"\"\"\n",
    "    try:\n",
    "        api = get_or_create_api(model_id)\n",
    "        status = training_status.get(model_id, \"not started\")\n",
    "        \n",
    "        return {\n",
    "            \"model_id\": model_id,\n",
    "            \"training_status\": status,\n",
    "            \"trained\": api.trained\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting training status: {str(e)}\")\n",
    "        raise HTTPException(status_code=404, detail=f\"Model {model_id} not found\")\n",
    "\n",
    "\n",
    "@app.post(\"/models/{model_id}/universal/optimize\")\n",
    "async def optimize_portfolio_endpoint(request: OptimizationRequest):\n",
    "    \"\"\"Optimize portfolio in universal mode\"\"\"\n",
    "    try:\n",
    "        api = get_or_create_api(request.model_id)\n",
    "        \n",
    "        if not api.trained:\n",
    "            raise HTTPException(status_code=400, detail=\"Model not trained. Train the model first.\")\n",
    "        \n",
    "        result = api.optimize_portfolio(\n",
    "            subset_tickers=request.subset_tickers,\n",
    "            strategy=request.strategy,\n",
    "            risk_tolerance=request.risk_tolerance\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error optimizing portfolio: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "@app.post(\"/models/{model_id}/local/optimize\")\n",
    "async def local_mode_optimize(request: LocalModeRequest):\n",
    "    \"\"\"Optimize portfolio in local mode using a binary mask\"\"\"\n",
    "    try:\n",
    "        api = get_or_create_api(request.model_id)\n",
    "        \n",
    "        if not api.trained:\n",
    "            raise HTTPException(status_code=400, detail=\"Model not trained. Train the model first.\")\n",
    "        \n",
    "        result = api.local_mode_predict(\n",
    "            asset_mask=request.asset_mask,\n",
    "            strategy_idx=request.strategy,\n",
    "            risk_tolerance=request.risk_tolerance\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in local mode optimization: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "@app.delete(\"/models/{model_id}\")\n",
    "async def delete_model(model_id: str):\n",
    "    \"\"\"Delete a model\"\"\"\n",
    "    try:\n",
    "        # Check if model exists\n",
    "        data_path = os.path.join(DATA_DIR, f\"{model_id}_data.pkl\")\n",
    "        model_path = os.path.join(MODEL_DIR, f\"{model_id}_model.pth\")\n",
    "        \n",
    "        files_deleted = 0\n",
    "        \n",
    "        # Remove from memory\n",
    "        if model_id in model_storage:\n",
    "            del model_storage[model_id]\n",
    "        \n",
    "        # Remove from disk\n",
    "        if os.path.exists(data_path):\n",
    "            os.remove(data_path)\n",
    "            files_deleted += 1\n",
    "        \n",
    "        if os.path.exists(model_path):\n",
    "            os.remove(model_path)\n",
    "            files_deleted += 1\n",
    "        \n",
    "        if files_deleted == 0:\n",
    "            raise HTTPException(status_code=404, detail=f\"Model {model_id} not found\")\n",
    "        \n",
    "        if model_id in training_status:\n",
    "            del training_status[model_id]\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": f\"Model {model_id} deleted\",\n",
    "            \"files_deleted\": files_deleted\n",
    "        }\n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error deleting model: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ac649-8077-43a7-a631-c9d3dfe56400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
